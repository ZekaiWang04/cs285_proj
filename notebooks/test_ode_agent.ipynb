{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler\n",
    "from cs285.envs.pendulum.pendulum_env import PendulumEnv\n",
    "from cs285.envs.dt_sampler import ConstantSampler\n",
    "from cs285.infrastructure.replay_buffer import ReplayBufferTrajectories\n",
    "from cs285.infrastructure.utils import sample_n_trajectories, RandomPolicy\n",
    "from cs285.agents.ode_agent import ODEAgent\n",
    "from cs285.agents.utils import save_leaves, load_leaves\n",
    "from typing import Callable, Optional, Tuple, Sequence\n",
    "import numpy as np\n",
    "import gym\n",
    "from cs285.infrastructure import pytorch_util as ptu\n",
    "from tqdm import trange\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "from diffrax import diffeqsolve, Dopri5\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.12it/s]\n",
      "100%|██████████| 2/2 [00:47<00:00, 23.57s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "mpc_dt_sampler = ConstantSampler(dt=0.05)\n",
    "agent_key, key = jax.random.split(key)\n",
    "neural_ode_name = \"vanilla\"\n",
    "neural_ode_kwargs = {\n",
    "    \"ode_dt0\": 0.005,\n",
    "    \"mlp_dynamics_setup\": {\n",
    "        \"hidden_size\":128,\n",
    "        \"num_layers\":4,\n",
    "        \"activation\":\"relu\",\n",
    "        \"output_activation\":\"identity\"\n",
    "    }\n",
    "}\n",
    "optimizer_name = \"adamw\"\n",
    "optimizer_kwargs = {\"learning_rate\": 1e-3}\n",
    "mb_agent = ODEAgent(\n",
    "    env=env,\n",
    "    key=agent_key,\n",
    "    neural_ode_name=neural_ode_name,\n",
    "    neural_ode_kwargs=neural_ode_kwargs,\n",
    "    optimizer_name=optimizer_name,\n",
    "    optimizer_kwargs=optimizer_kwargs,\n",
    "    ensemble_size=10,\n",
    "    train_discount=1,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_dt_sampler=mpc_dt_sampler,\n",
    "    mpc_strategy=\"cem\",\n",
    "    mpc_discount=0.9,\n",
    "    mpc_num_action_sequences=1000,\n",
    "    cem_num_iters=4,\n",
    "    cem_num_elites=5,\n",
    "    cem_alpha=1,\n",
    ")\n",
    "replay_buffer = ReplayBufferTrajectories(seed=42)\n",
    "trajs, _ = sample_n_trajectories(env, RandomPolicy(env=env), ntraj=10, max_length=200, key=key)\n",
    "replay_buffer.add_rollouts(trajs)\n",
    "\n",
    "batch_size = 64\n",
    "for n in trange(2):\n",
    "    for i in range(mb_agent.ensemble_size):\n",
    "        obs, acs, times = jnp.empty(shape=(batch_size, 201, 3)), jnp.empty(shape=(batch_size, 201, 1)), jnp.empty(shape=(batch_size, 201))\n",
    "        for m in range(batch_size):\n",
    "            traj = replay_buffer.sample_rollout()\n",
    "            obs.at[m].set(traj[\"observations\"])\n",
    "            acs.at[m].set(traj[\"actions\"])\n",
    "            times.at[m].set(jnp.cumsum(traj[\"dts\"]))\n",
    "        loss = mb_agent.batched_update(i=i, obs=obs, acs=acs, times=times)\n",
    "# 24 seconds, might need profiler to see where I can imporve\n",
    "\n",
    "trajs, _ = sample_n_trajectories(\n",
    "    env=env,\n",
    "    policy=mb_agent,\n",
    "    ntraj=10,\n",
    "    max_length=200,\n",
    "    key=key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.19it/s]\n",
      "100%|██████████| 2/2 [00:47<00:00, 23.99s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "mpc_dt_sampler = ConstantSampler(dt=0.05)\n",
    "agent_key, key = jax.random.split(key)\n",
    "neural_ode_name = \"augmented\"\n",
    "neural_ode_kwargs = {\n",
    "    \"ode_dt0\": 0.005,\n",
    "    \"mlp_dynamics_setup\": {\n",
    "        \"hidden_size\":128,\n",
    "        \"num_layers\":4,\n",
    "        \"activation\":\"relu\",\n",
    "        \"output_activation\":\"identity\"\n",
    "    },\n",
    "    \"aug_dim\": 3\n",
    "}\n",
    "optimizer_name = \"adamw\"\n",
    "optimizer_kwargs = {\"learning_rate\": 1e-3}\n",
    "mb_agent = ODEAgent(\n",
    "    env=env,\n",
    "    key=agent_key,\n",
    "    neural_ode_name=neural_ode_name,\n",
    "    neural_ode_kwargs=neural_ode_kwargs,\n",
    "    optimizer_name=optimizer_name,\n",
    "    optimizer_kwargs=optimizer_kwargs,\n",
    "    ensemble_size=10,\n",
    "    train_discount=1,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_dt_sampler=mpc_dt_sampler,\n",
    "    mpc_strategy=\"cem\",\n",
    "    mpc_discount=0.9,\n",
    "    mpc_num_action_sequences=1000,\n",
    "    cem_num_iters=4,\n",
    "    cem_num_elites=5,\n",
    "    cem_alpha=1,\n",
    ")\n",
    "replay_buffer = ReplayBufferTrajectories(seed=42)\n",
    "trajs, _ = sample_n_trajectories(env, RandomPolicy(env=env), ntraj=10, max_length=200, key=key)\n",
    "replay_buffer.add_rollouts(trajs)\n",
    "\n",
    "batch_size = 64\n",
    "for n in trange(2):\n",
    "    for i in range(mb_agent.ensemble_size):\n",
    "        obs, acs, times = jnp.empty(shape=(batch_size, 201, 3)), jnp.empty(shape=(batch_size, 201, 1)), jnp.empty(shape=(batch_size, 201))\n",
    "        for m in range(batch_size):\n",
    "            traj = replay_buffer.sample_rollout()\n",
    "            obs.at[m].set(traj[\"observations\"])\n",
    "            acs.at[m].set(traj[\"actions\"])\n",
    "            times.at[m].set(jnp.cumsum(traj[\"dts\"]))\n",
    "        loss = mb_agent.batched_update(i=i, obs=obs, acs=acs, times=times)\n",
    "# 24 seconds, might need profiler to see where I can imporve\n",
    "\n",
    "trajs, _ = sample_n_trajectories(\n",
    "    env=env,\n",
    "    policy=mb_agent,\n",
    "    ntraj=10,\n",
    "    max_length=200,\n",
    "    key=key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.62it/s]\n",
      "100%|██████████| 2/2 [01:01<00:00, 30.82s/it]\n",
      "100%|██████████| 10/10 [00:21<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "mpc_dt_sampler = ConstantSampler(dt=0.05)\n",
    "agent_key, key = jax.random.split(key)\n",
    "mlp_setup = {\n",
    "    \"hidden_size\":128,\n",
    "    \"num_layers\":4,\n",
    "    \"activation\":\"relu\",\n",
    "    \"output_activation\":\"identity\"\n",
    "}\n",
    "neural_ode_name = \"latent_mlp\"\n",
    "neural_ode_kwargs = {\n",
    "    \"ode_dt0\": 0.005,\n",
    "    \"ac_latent_dim\": 4,\n",
    "    \"ob_latent_dim\": 4,\n",
    "    \"mlp_dynamics_setup\": mlp_setup,\n",
    "    \"mlp_ob_encoder_setup\": mlp_setup,\n",
    "    \"mlp_ob_decoder_setup\": mlp_setup,\n",
    "    \"mlp_ac_encoder_setup\": mlp_setup,\n",
    "}\n",
    "optimizer_name = \"adamw\"\n",
    "optimizer_kwargs = {\"learning_rate\": 1e-3}\n",
    "mb_agent = ODEAgent(\n",
    "    env=env,\n",
    "    key=agent_key,\n",
    "    neural_ode_name=neural_ode_name,\n",
    "    neural_ode_kwargs=neural_ode_kwargs,\n",
    "    optimizer_name=optimizer_name,\n",
    "    optimizer_kwargs=optimizer_kwargs,\n",
    "    ensemble_size=10,\n",
    "    train_discount=1,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_dt_sampler=mpc_dt_sampler,\n",
    "    mpc_strategy=\"cem\",\n",
    "    mpc_discount=0.9,\n",
    "    mpc_num_action_sequences=1000,\n",
    "    cem_num_iters=4,\n",
    "    cem_num_elites=5,\n",
    "    cem_alpha=1,\n",
    ")\n",
    "replay_buffer = ReplayBufferTrajectories(seed=42)\n",
    "trajs, _ = sample_n_trajectories(env, RandomPolicy(env=env), ntraj=10, max_length=200, key=key)\n",
    "replay_buffer.add_rollouts(trajs)\n",
    "\n",
    "batch_size = 64\n",
    "for n in trange(2):\n",
    "    for i in range(mb_agent.ensemble_size):\n",
    "        obs, acs, times = jnp.empty(shape=(batch_size, 201, 3)), jnp.empty(shape=(batch_size, 201, 1)), jnp.empty(shape=(batch_size, 201))\n",
    "        for m in range(batch_size):\n",
    "            traj = replay_buffer.sample_rollout()\n",
    "            obs.at[m].set(traj[\"observations\"])\n",
    "            acs.at[m].set(traj[\"actions\"])\n",
    "            times.at[m].set(jnp.cumsum(traj[\"dts\"]))\n",
    "        loss = mb_agent.batched_update(i=i, obs=obs, acs=acs, times=times)\n",
    "# 24 seconds, might need profiler to see where I can imporve\n",
    "\n",
    "trajs, _ = sample_n_trajectories(\n",
    "    env=env,\n",
    "    policy=mb_agent,\n",
    "    ntraj=10,\n",
    "    max_length=200,\n",
    "    key=key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.26it/s]\n",
      "100%|██████████| 2/2 [01:27<00:00, 43.55s/it]\n",
      "100%|██████████| 10/10 [00:35<00:00,  3.55s/it]\n"
     ]
    }
   ],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "mpc_dt_sampler = ConstantSampler(dt=0.05)\n",
    "agent_key, key = jax.random.split(key)\n",
    "mlp_setup = {\n",
    "    \"hidden_size\":128,\n",
    "    \"num_layers\":4,\n",
    "    \"activation\":\"relu\",\n",
    "    \"output_activation\":\"identity\"\n",
    "}\n",
    "neural_ode_name = \"ode_rnn\"\n",
    "neural_ode_kwargs = {\n",
    "    \"ode_dt0\": 0.005,\n",
    "    \"latent_dim\": 4,\n",
    "    \"rnn_type\": \"lstm\",\n",
    "    \"mlp_dynamics_setup\": mlp_setup,\n",
    "    \"mlp_ob_encoder_setup\": mlp_setup,\n",
    "    \"mlp_ob_decoder_setup\": mlp_setup,\n",
    "}\n",
    "optimizer_name = \"adamw\"\n",
    "optimizer_kwargs = {\"learning_rate\": 1e-3}\n",
    "mb_agent = ODEAgent(\n",
    "    env=env,\n",
    "    key=agent_key,\n",
    "    neural_ode_name=neural_ode_name,\n",
    "    neural_ode_kwargs=neural_ode_kwargs,\n",
    "    optimizer_name=optimizer_name,\n",
    "    optimizer_kwargs=optimizer_kwargs,\n",
    "    ensemble_size=10,\n",
    "    train_discount=1,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_dt_sampler=mpc_dt_sampler,\n",
    "    mpc_strategy=\"cem\",\n",
    "    mpc_discount=0.9,\n",
    "    mpc_num_action_sequences=1000,\n",
    "    cem_num_iters=4,\n",
    "    cem_num_elites=5,\n",
    "    cem_alpha=1,\n",
    ")\n",
    "replay_buffer = ReplayBufferTrajectories(seed=42)\n",
    "trajs, _ = sample_n_trajectories(env, RandomPolicy(env=env), ntraj=10, max_length=200, key=key)\n",
    "replay_buffer.add_rollouts(trajs)\n",
    "\n",
    "batch_size = 64\n",
    "for n in trange(2):\n",
    "    for i in range(mb_agent.ensemble_size):\n",
    "        obs, acs, times = jnp.empty(shape=(batch_size, 201, 3)), jnp.empty(shape=(batch_size, 201, 1)), jnp.empty(shape=(batch_size, 201))\n",
    "        for m in range(batch_size):\n",
    "            traj = replay_buffer.sample_rollout()\n",
    "            obs.at[m].set(traj[\"observations\"])\n",
    "            acs.at[m].set(traj[\"actions\"])\n",
    "            times.at[m].set(jnp.cumsum(traj[\"dts\"]))\n",
    "        loss = mb_agent.batched_update(i=i, obs=obs, acs=acs, times=times)\n",
    "# 24 seconds, might need profiler to see where I can imporve\n",
    "\n",
    "trajs, _ = sample_n_trajectories(\n",
    "    env=env,\n",
    "    policy=mb_agent,\n",
    "    ntraj=10,\n",
    "    max_length=200,\n",
    "    key=key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.96it/s]\n",
      "100%|██████████| 2/2 [01:23<00:00, 41.93s/it]\n",
      "100%|██████████| 10/10 [00:36<00:00,  3.65s/it]\n"
     ]
    }
   ],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "mpc_dt_sampler = ConstantSampler(dt=0.05)\n",
    "agent_key, key = jax.random.split(key)\n",
    "mlp_setup = {\n",
    "    \"hidden_size\":128,\n",
    "    \"num_layers\":4,\n",
    "    \"activation\":\"relu\",\n",
    "    \"output_activation\":\"identity\"\n",
    "}\n",
    "neural_ode_name = \"ode_rnn\"\n",
    "neural_ode_kwargs = {\n",
    "    \"ode_dt0\": 0.005,\n",
    "    \"latent_dim\": 4,\n",
    "    \"rnn_type\": \"gru\",\n",
    "    \"mlp_dynamics_setup\": mlp_setup,\n",
    "    \"mlp_ob_encoder_setup\": mlp_setup,\n",
    "    \"mlp_ob_decoder_setup\": mlp_setup,\n",
    "}\n",
    "optimizer_name = \"adamw\"\n",
    "optimizer_kwargs = {\"learning_rate\": 1e-3}\n",
    "mb_agent = ODEAgent(\n",
    "    env=env,\n",
    "    key=agent_key,\n",
    "    neural_ode_name=neural_ode_name,\n",
    "    neural_ode_kwargs=neural_ode_kwargs,\n",
    "    optimizer_name=optimizer_name,\n",
    "    optimizer_kwargs=optimizer_kwargs,\n",
    "    ensemble_size=10,\n",
    "    train_discount=1,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_dt_sampler=mpc_dt_sampler,\n",
    "    mpc_strategy=\"cem\",\n",
    "    mpc_discount=0.9,\n",
    "    mpc_num_action_sequences=1000,\n",
    "    cem_num_iters=4,\n",
    "    cem_num_elites=5,\n",
    "    cem_alpha=1,\n",
    ")\n",
    "replay_buffer = ReplayBufferTrajectories(seed=42)\n",
    "trajs, _ = sample_n_trajectories(env, RandomPolicy(env=env), ntraj=10, max_length=200, key=key)\n",
    "replay_buffer.add_rollouts(trajs)\n",
    "\n",
    "batch_size = 64\n",
    "for n in trange(2):\n",
    "    for i in range(mb_agent.ensemble_size):\n",
    "        obs, acs, times = jnp.empty(shape=(batch_size, 201, 3)), jnp.empty(shape=(batch_size, 201, 1)), jnp.empty(shape=(batch_size, 201))\n",
    "        for m in range(batch_size):\n",
    "            traj = replay_buffer.sample_rollout()\n",
    "            obs.at[m].set(traj[\"observations\"])\n",
    "            acs.at[m].set(traj[\"actions\"])\n",
    "            times.at[m].set(jnp.cumsum(traj[\"dts\"]))\n",
    "        loss = mb_agent.batched_update(i=i, obs=obs, acs=acs, times=times)\n",
    "# 24 seconds, might need profiler to see where I can imporve\n",
    "\n",
    "trajs, _ = sample_n_trajectories(\n",
    "    env=env,\n",
    "    policy=mb_agent,\n",
    "    ntraj=10,\n",
    "    max_length=200,\n",
    "    key=key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
