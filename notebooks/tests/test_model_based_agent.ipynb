{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from cs285.envs.pendulum.pendulum_env import PendulumEnv\n",
    "from cs285.envs.dt_sampler import ConstantSampler\n",
    "from cs285.infrastructure.replay_buffer import ReplayBufferTransitions\n",
    "from cs285.infrastructure.utils import sample_n_trajectories, RandomPolicy\n",
    "from cs285.agents.model_based_agent import ModelBasedAgent\n",
    "from typing import Callable, Optional, Tuple, Sequence\n",
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import trange\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "from diffrax import diffeqsolve, Dopri5\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.86it/s]\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.03it/s]\n",
      "100%|██████████| 10/10 [23:04<00:00, 138.40s/it]\n"
     ]
    }
   ],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "mpc_dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "num_layers = 4\n",
    "hidden_size = 128\n",
    "lr=0.001\n",
    "agent_key, key = jax.random.split(key)\n",
    "\n",
    "mb_agent = ModelBasedAgent(\n",
    "    env=env,\n",
    "    key=agent_key,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    activation=\"relu\",\n",
    "    output_activation=\"identity\",\n",
    "    lr=lr,\n",
    "    ensemble_size=10,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_strategy=\"random\",\n",
    "    mpc_discount=1.0,\n",
    "    mpc_num_action_sequences=100,\n",
    "    mpc_dt_sampler=mpc_dt_sampler,\n",
    "    cem_num_iters=4,\n",
    "    cem_num_elites=5,\n",
    "    cem_alpha=1,\n",
    "    mode=\"vanilla\"\n",
    ")\n",
    "replay_buffer = ReplayBufferTransitions()\n",
    "trajs, _ = sample_n_trajectories(env, RandomPolicy(env=env), ntraj=10, max_length=200, key=key)\n",
    "for traj in trajs:\n",
    "    replay_buffer.batched_insert(\n",
    "    observations=traj[\"observation\"],\n",
    "    actions=traj[\"action\"],\n",
    "    rewards=traj[\"reward\"],\n",
    "    next_observations=traj[\"next_observation\"],\n",
    "    dones=traj[\"done\"],\n",
    "    dts=traj[\"dt\"]\n",
    ")\n",
    "\n",
    "mb_agent.update_statistics(\n",
    "    obs=replay_buffer.observations,\n",
    "    acs=replay_buffer.actions,\n",
    "    next_obs = replay_buffer.next_observations\n",
    ")\n",
    "\n",
    "for n in trange(10):\n",
    "    for i in range(mb_agent.ensemble_size):\n",
    "        batch = replay_buffer.sample(64)\n",
    "        mb_agent.batched_update(i, batch[\"observations\"], batch[\"actions\"], batch[\"next_observations\"], batch[\"dts\"])\n",
    "\n",
    "trajs, _ = sample_n_trajectories(\n",
    "    env=env,\n",
    "    policy=mb_agent,\n",
    "    ntraj=10,\n",
    "    max_length=200,\n",
    "    key=key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.60it/s]\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.03it/s]\n",
      "100%|██████████| 10/10 [23:03<00:00, 138.38s/it]\n"
     ]
    }
   ],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "mpc_dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "num_layers = 4\n",
    "hidden_size = 128\n",
    "lr=0.001\n",
    "agent_key, key = jax.random.split(key)\n",
    "\n",
    "mb_agent = ModelBasedAgent(\n",
    "    env=env,\n",
    "    key=agent_key,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    activation=\"relu\",\n",
    "    output_activation=\"identity\",\n",
    "    lr=lr,\n",
    "    ensemble_size=10,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_strategy=\"random\",\n",
    "    mpc_discount=1.0,\n",
    "    mpc_num_action_sequences=100,\n",
    "    mpc_dt_sampler=mpc_dt_sampler,\n",
    "    cem_num_iters=4,\n",
    "    cem_num_elites=5,\n",
    "    cem_alpha=1,\n",
    "    mode=\"mul_dt\"\n",
    ")\n",
    "replay_buffer = ReplayBufferTransitions()\n",
    "trajs, _ = sample_n_trajectories(env, RandomPolicy(env=env), ntraj=10, max_length=200, key=key)\n",
    "for traj in trajs:\n",
    "    replay_buffer.batched_insert(\n",
    "    observations=traj[\"observation\"],\n",
    "    actions=traj[\"action\"],\n",
    "    rewards=traj[\"reward\"],\n",
    "    next_observations=traj[\"next_observation\"],\n",
    "    dones=traj[\"done\"],\n",
    "    dts=traj[\"dt\"]\n",
    ")\n",
    "\n",
    "mb_agent.update_statistics(\n",
    "    obs=replay_buffer.observations,\n",
    "    acs=replay_buffer.actions,\n",
    "    next_obs = replay_buffer.next_observations\n",
    ")\n",
    "\n",
    "for n in trange(10):\n",
    "    for i in range(mb_agent.ensemble_size):\n",
    "        batch = replay_buffer.sample(64)\n",
    "        mb_agent.batched_update(i, batch[\"observations\"], batch[\"actions\"], batch[\"next_observations\"], batch[\"dts\"])\n",
    "\n",
    "trajs, _ = sample_n_trajectories(\n",
    "    env=env,\n",
    "    policy=mb_agent,\n",
    "    ntraj=10,\n",
    "    max_length=200,\n",
    "    key=key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.58it/s]\n",
      "100%|██████████| 10/10 [00:09<00:00,  1.07it/s]\n",
      "100%|██████████| 10/10 [27:31<00:00, 165.12s/it]\n"
     ]
    }
   ],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "mpc_dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "num_layers = 4\n",
    "hidden_size = 128\n",
    "lr=0.001\n",
    "agent_key, key = jax.random.split(key)\n",
    "\n",
    "mb_agent = ModelBasedAgent(\n",
    "    env=env,\n",
    "    key=agent_key,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    activation=\"relu\",\n",
    "    output_activation=\"identity\",\n",
    "    lr=lr,\n",
    "    ensemble_size=10,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_strategy=\"random\",\n",
    "    mpc_discount=1.0,\n",
    "    mpc_num_action_sequences=100,\n",
    "    mpc_dt_sampler=mpc_dt_sampler,\n",
    "    cem_num_iters=4,\n",
    "    cem_num_elites=5,\n",
    "    cem_alpha=1,\n",
    "    mode=\"dt_in\"\n",
    ")\n",
    "replay_buffer = ReplayBufferTransitions()\n",
    "trajs, _ = sample_n_trajectories(env, RandomPolicy(env=env), ntraj=10, max_length=200, key=key)\n",
    "for traj in trajs:\n",
    "    replay_buffer.batched_insert(\n",
    "    observations=traj[\"observation\"],\n",
    "    actions=traj[\"action\"],\n",
    "    rewards=traj[\"reward\"],\n",
    "    next_observations=traj[\"next_observation\"],\n",
    "    dones=traj[\"done\"],\n",
    "    dts=traj[\"dt\"]\n",
    ")\n",
    "\n",
    "mb_agent.update_statistics(\n",
    "    obs=replay_buffer.observations,\n",
    "    acs=replay_buffer.actions,\n",
    "    next_obs = replay_buffer.next_observations\n",
    ")\n",
    "\n",
    "for n in trange(10):\n",
    "    for i in range(mb_agent.ensemble_size):\n",
    "        batch = replay_buffer.sample(64)\n",
    "        mb_agent.batched_update(i, batch[\"observations\"], batch[\"actions\"], batch[\"next_observations\"], batch[\"dts\"])\n",
    "\n",
    "trajs, _ = sample_n_trajectories(\n",
    "    env=env,\n",
    "    policy=mb_agent,\n",
    "    ntraj=10,\n",
    "    max_length=200,\n",
    "    key=key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
