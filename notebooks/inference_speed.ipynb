{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler\n",
    "from cs285.envs.pendulum.pendulum_env import PendulumEnv\n",
    "from cs285.envs.dt_sampler import ConstantSampler\n",
    "from cs285.infrastructure.replay_buffer import ReplayBufferTrajectories\n",
    "from cs285.infrastructure.utils import sample_n_trajectories, RandomPolicy\n",
    "from typing import Callable, Optional, Tuple, Sequence\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import gym\n",
    "from cs285.infrastructure import pytorch_util as ptu\n",
    "from torchdiffeq import odeint\n",
    "from tqdm import trange\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "from diffrax import diffeqsolve, Dopri5\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = eqx.nn.MLP(in_size=3+1, out_size=3, width_size=32, depth=3, key=key)\n",
    "def f(t, y, args):\n",
    "    times = args[0] # args[\"times\"] # (ep_len,)\n",
    "    actions = args[1] # args[\"actions\"] # (ep_len, ac_dim)\n",
    "    idx = jnp.searchsorted(times, t, side=\"right\") - 1\n",
    "    action = actions[idx] # (ac_dim)\n",
    "    # althoug I believe this should also work for batched \n",
    "    return mlp(jnp.concatenate((y, action), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros_like requires ndarray or scalar arguments, got <class 'jax._src.custom_derivatives.custom_jvp'> at position 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m solver \u001b[39m=\u001b[39m Dopri5()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m optim \u001b[39m=\u001b[39m optax\u001b[39m.\u001b[39madamw(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m optim_state \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39;49minit(mlp)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# @eqx.filter_jit\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m@eqx\u001b[39m\u001b[39m.\u001b[39mfilter_value_and_grad\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvag\u001b[39m(mlp):\n",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/optax/_src/combine.py:50\u001b[0m, in \u001b[0;36mchain.<locals>.init_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_fn\u001b[39m(params):\n\u001b[0;32m---> 50\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(fn(params) \u001b[39mfor\u001b[39;49;00m fn \u001b[39min\u001b[39;49;00m init_fns)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/optax/_src/combine.py:50\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_fn\u001b[39m(params):\n\u001b[0;32m---> 50\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(fn(params) \u001b[39mfor\u001b[39;00m fn \u001b[39min\u001b[39;00m init_fns)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/optax/_src/transform.py:335\u001b[0m, in \u001b[0;36mscale_by_adam.<locals>.init_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_fn\u001b[39m(params):\n\u001b[0;32m--> 335\u001b[0m   mu \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mtree_util\u001b[39m.\u001b[39;49mtree_map(  \u001b[39m# First moment\u001b[39;49;00m\n\u001b[1;32m    336\u001b[0m       \u001b[39mlambda\u001b[39;49;00m t: jnp\u001b[39m.\u001b[39;49mzeros_like(t, dtype\u001b[39m=\u001b[39;49mmu_dtype), params)\n\u001b[1;32m    337\u001b[0m   nu \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mtree_util\u001b[39m.\u001b[39mtree_map(jnp\u001b[39m.\u001b[39mzeros_like, params)  \u001b[39m# Second moment\u001b[39;00m\n\u001b[1;32m    338\u001b[0m   \u001b[39mreturn\u001b[39;00m ScaleByAdamState(count\u001b[39m=\u001b[39mjnp\u001b[39m.\u001b[39mzeros([], jnp\u001b[39m.\u001b[39mint32), mu\u001b[39m=\u001b[39mmu, nu\u001b[39m=\u001b[39mnu)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/tree_util.py:244\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    242\u001b[0m leaves, treedef \u001b[39m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[1;32m    243\u001b[0m all_leaves \u001b[39m=\u001b[39m [leaves] \u001b[39m+\u001b[39m [treedef\u001b[39m.\u001b[39mflatten_up_to(r) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m rest]\n\u001b[0;32m--> 244\u001b[0m \u001b[39mreturn\u001b[39;00m treedef\u001b[39m.\u001b[39;49munflatten(f(\u001b[39m*\u001b[39;49mxs) \u001b[39mfor\u001b[39;49;00m xs \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49mall_leaves))\n",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/tree_util.py:244\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    242\u001b[0m leaves, treedef \u001b[39m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[1;32m    243\u001b[0m all_leaves \u001b[39m=\u001b[39m [leaves] \u001b[39m+\u001b[39m [treedef\u001b[39m.\u001b[39mflatten_up_to(r) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m rest]\n\u001b[0;32m--> 244\u001b[0m \u001b[39mreturn\u001b[39;00m treedef\u001b[39m.\u001b[39munflatten(f(\u001b[39m*\u001b[39;49mxs) \u001b[39mfor\u001b[39;00m xs \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mall_leaves))\n",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/optax/_src/transform.py:336\u001b[0m, in \u001b[0;36mscale_by_adam.<locals>.init_fn.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_fn\u001b[39m(params):\n\u001b[1;32m    335\u001b[0m   mu \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mtree_util\u001b[39m.\u001b[39mtree_map(  \u001b[39m# First moment\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m       \u001b[39mlambda\u001b[39;00m t: jnp\u001b[39m.\u001b[39;49mzeros_like(t, dtype\u001b[39m=\u001b[39;49mmu_dtype), params)\n\u001b[1;32m    337\u001b[0m   nu \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mtree_util\u001b[39m.\u001b[39mtree_map(jnp\u001b[39m.\u001b[39mzeros_like, params)  \u001b[39m# Second moment\u001b[39;00m\n\u001b[1;32m    338\u001b[0m   \u001b[39mreturn\u001b[39;00m ScaleByAdamState(count\u001b[39m=\u001b[39mjnp\u001b[39m.\u001b[39mzeros([], jnp\u001b[39m.\u001b[39mint32), mu\u001b[39m=\u001b[39mmu, nu\u001b[39m=\u001b[39mnu)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:2215\u001b[0m, in \u001b[0;36mzeros_like\u001b[0;34m(a, dtype, shape)\u001b[0m\n\u001b[1;32m   2210\u001b[0m \u001b[39m@util\u001b[39m\u001b[39m.\u001b[39m_wraps(np\u001b[39m.\u001b[39mzeros_like)\n\u001b[1;32m   2211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mzeros_like\u001b[39m(a: ArrayLike \u001b[39m|\u001b[39m DuckTypedArray,\n\u001b[1;32m   2212\u001b[0m                dtype: DTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2213\u001b[0m                shape: Any \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[1;32m   2214\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mhasattr\u001b[39m(a, \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(a, \u001b[39m'\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m'\u001b[39m)):  \u001b[39m# support duck typing\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     util\u001b[39m.\u001b[39;49mcheck_arraylike(\u001b[39m\"\u001b[39;49m\u001b[39mzeros_like\u001b[39;49m\u001b[39m\"\u001b[39;49m, a)\n\u001b[1;32m   2216\u001b[0m   dtypes\u001b[39m.\u001b[39mcheck_user_dtype_supported(dtype, \u001b[39m\"\u001b[39m\u001b[39mzeros_like\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2217\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/numpy/util.py:335\u001b[0m, in \u001b[0;36mcheck_arraylike\u001b[0;34m(fun_name, emit_warning, stacklevel, *args)\u001b[0m\n\u001b[1;32m    332\u001b[0m   warnings\u001b[39m.\u001b[39mwarn(msg \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m In a future JAX release this will be an error.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    333\u001b[0m                 category\u001b[39m=\u001b[39m\u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39mstacklevel)\n\u001b[1;32m    334\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(fun_name, \u001b[39mtype\u001b[39m(arg), pos))\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros_like requires ndarray or scalar arguments, got <class 'jax._src.custom_derivatives.custom_jvp'> at position 0."
     ]
    }
   ],
   "source": [
    "times = jnp.linspace(0, 1, 100)\n",
    "actions = jax.random.normal(key=key, shape=(100,1))\n",
    "observations = jax.random.normal(key=key, shape=(100,3))\n",
    "args = [times, actions]\n",
    "solver = Dopri5()\n",
    "optim = optax.adamw(learning_rate=0.01)\n",
    "optim_state = optim.init(mlp)\n",
    "\n",
    "# @eqx.filter_jit\n",
    "@eqx.filter_value_and_grad\n",
    "def vag(mlp):\n",
    "    def f(t, y, args):\n",
    "        times = args[0] # args[\"times\"] # (ep_len,)\n",
    "        actions = args[1] # args[\"actions\"] # (ep_len, ac_dim)\n",
    "        idx = jnp.searchsorted(times, t, side=\"right\") - 1\n",
    "        action = actions[idx] # (ac_dim)\n",
    "        # althoug I believe this should also work for batched \n",
    "        return mlp(jnp.concatenate((y, action), axis=-1))\n",
    "    ode_out = diffeqsolve(terms=diffrax.ODETerm(f),\n",
    "                        solver=solver,\n",
    "                        t0=times[0],\n",
    "                        t1=times[-1],\n",
    "                        dt0=0.005,\n",
    "                        y0=observations[0],\n",
    "                        saveat=diffrax.SaveAt(ts=times),\n",
    "                        args=args)\n",
    "    return jnp.mean((ode_out.ys - observations) ** 2)\n",
    "\n",
    "value, grad = vag(mlp)\n",
    "updates, opt_state = optim.update(grad, optim_state)\n",
    "eqx.apply_updates(mlp, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE_jax(eqx.Module):\n",
    "    _str_to_activation = {\n",
    "        \"relu\": jax.nn.relu,\n",
    "        \"tanh\": jax.nn.tanh,\n",
    "        \"leaky_relu\": jax.nn.leaky_relu,\n",
    "        \"sigmoid\": jax.nn.sigmoid,\n",
    "        \"selu\": jax.nn.selu,\n",
    "        \"softplus\": jax.nn.softplus,\n",
    "        \"identity\": lambda x: x,\n",
    "    }\n",
    "    mlp: eqx.nn.MLP\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            ob_dim,\n",
    "            ac_dim,\n",
    "            key,\n",
    "            activation=\"relu\",\n",
    "            output_activation=\"identity\",\n",
    "        ):\n",
    "        super().__init__()\n",
    "        activation = self._str_to_activation[activation]\n",
    "        output_activation = self._str_to_activation[output_activation]\n",
    "        # hidden_size is an integer\n",
    "        self.mlp = eqx.nn.MLP(in_size=ob_dim+ac_dim,\n",
    "                              out_size=ob_dim,\n",
    "                              width_size=hidden_size,\n",
    "                              depth=num_layers,\n",
    "                              activation=activation,\n",
    "                              final_activation=output_activation,\n",
    "                              key=key)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, t, y, args):\n",
    "        # args is a dictionary that contains times and actions\n",
    "        times = args[\"times\"] # (ep_len,)\n",
    "        actions = args[\"actions\"] # (ep_len, ac_dim)\n",
    "        idx = jnp.searchsorted(times, t, side=\"right\") - 1\n",
    "        action = actions[idx] # (ac_dim)\n",
    "        # althoug I believe this should also work for batched \n",
    "        return self.mlp(jnp.concatenate((y, action), axis=-1))\n",
    "    \n",
    "class ODEAgent_jax():\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        key,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        ensemble_size: int,\n",
    "        train_timestep: float,\n",
    "        mpc_horizon_steps: int,\n",
    "        mpc_timestep: float,\n",
    "        mpc_strategy: str,\n",
    "        mpc_num_action_sequences: int,\n",
    "        cem_num_iters: Optional[int] = None,\n",
    "        cem_num_elites: Optional[int] = None,\n",
    "        cem_alpha: Optional[float] = None,\n",
    "        activation: str = \"relu\",\n",
    "        output_activation: str = \"identity\",\n",
    "        lr: float=0.001\n",
    "    ):\n",
    "        # super().__init__()\n",
    "        self.env = env\n",
    "        self.train_timestep = train_timestep\n",
    "        self.mpc_horizon_steps = mpc_horizon_steps # in terms of timesteps\n",
    "        self.mpc_strategy = mpc_strategy\n",
    "        self.mpc_num_action_sequences = mpc_num_action_sequences\n",
    "        self.cem_num_iters = cem_num_iters\n",
    "        self.cem_num_elites = cem_num_elites\n",
    "        self.cem_alpha = cem_alpha\n",
    "        self.mpc_timestep = mpc_timestep # when evaluating\n",
    "\n",
    "        assert mpc_strategy in (\n",
    "            \"random\",\n",
    "            \"cem\",\n",
    "        ), f\"'{mpc_strategy}' is not a valid MPC strategy\"\n",
    "\n",
    "        # ensure the environment is state-based\n",
    "        assert len(env.observation_space.shape) == 1\n",
    "        assert len(env.action_space.shape) == 1\n",
    "\n",
    "        self.ob_dim = env.observation_space.shape[0]\n",
    "        self.ac_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.ensemble_size = ensemble_size\n",
    "        keys = jax.random.split(key, ensemble_size)\n",
    "        self.ode_functions = [NeuralODE_jax(\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            ob_dim=self.ob_dim,\n",
    "            ac_dim=self.ac_dim,\n",
    "            activation=activation,\n",
    "            output_activation=output_activation,\n",
    "            key = keys[n]\n",
    "            ) for n in range(ensemble_size)]\n",
    "        self.optims = [optax.adamw(lr) for _ in range(ensemble_size)]\n",
    "        self.optim_states = [self.optims[n].init(eqx.filter(self.ode_functions[n], eqx.is_array)) for n in range(self.ensemble_size)]\n",
    "\n",
    "        self.solver = Dopri5()\n",
    "    \n",
    "    def update(self, i: int, obs: np.ndarray, acs: np.ndarray, times: np.ndarray):\n",
    "        \"\"\"\n",
    "        Update self.dynamics_models[i] using the given trajectory\n",
    "\n",
    "        Args:\n",
    "            i: index of the dynamics model to update\n",
    "            obs: (ep_len, ob_dim)\n",
    "            acs: (ep_len, ac_dim)\n",
    "            times: (ep_len)\n",
    "        \"\"\"\n",
    "        @eqx.filter_value_and_grad\n",
    "        def loss_grad(ode_func):\n",
    "            sol = diffeqsolve(\n",
    "                diffrax.ODETerm(ode_func), \n",
    "                self.solver, \n",
    "                t0=times[0], \n",
    "                t1=times[-1], \n",
    "                dt0=self.train_timestep,\n",
    "                y0 = obs[0, :],\n",
    "                args={\"times\": times, \"actions\": acs},\n",
    "                saveat=diffrax.SaveAt(ts=times)\n",
    "            )\n",
    "            assert sol.ys.shape == obs.shape\n",
    "            return jnp.mean((sol.ys - obs) ** 2) # do we want a  \"discount\"-like trick\n",
    "\n",
    "        @eqx.filter_jit\n",
    "        def make_step(ode_func, optim, opt_state):\n",
    "            loss, grad = loss_grad(ode_func)\n",
    "            updates, opt_state = optim.update(grad, opt_state)\n",
    "            ode_func = eqx.apply_updates(ode_func, updates)\n",
    "            return loss, ode_func, opt_state\n",
    "        \n",
    "        ode_func, optim, opt_state = self.ode_functions[i], self.optims[i], self.optim_states[i]\n",
    "        loss, ode_func, opt_state = make_step(ode_func, optim, opt_state)\n",
    "        self.ode_functions[i], self.optim_states[i] = ode_func, opt_state\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 56.87it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are using a transformation that requires the current value of parameters, but you are not passing `params` when calling `update`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(mb_agent_jas\u001b[39m.\u001b[39mensemble_size):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     traj \u001b[39m=\u001b[39m replay_buffer\u001b[39m.\u001b[39msample_rollout()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     mb_agent_jas\u001b[39m.\u001b[39;49mupdate(i, traj[\u001b[39m\"\u001b[39;49m\u001b[39mobservations\u001b[39;49m\u001b[39m\"\u001b[39;49m], traj[\u001b[39m\"\u001b[39;49m\u001b[39mactions\u001b[39;49m\u001b[39m\"\u001b[39;49m], jnp\u001b[39m.\u001b[39;49mcumsum(traj[\u001b[39m\"\u001b[39;49m\u001b[39mdts\u001b[39;49m\u001b[39m\"\u001b[39;49m]))\n",
      "\u001b[1;32m/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss, ode_func, opt_state\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m ode_func, optim, opt_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mode_functions[i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptims[i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_states[i]\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m loss, ode_func, opt_state \u001b[39m=\u001b[39m make_step(ode_func, optim, opt_state)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mode_functions[i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_states[i] \u001b[39m=\u001b[39m ode_func, opt_state\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mitem()\n",
      "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "\u001b[1;32m/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m \u001b[39m@eqx\u001b[39m\u001b[39m.\u001b[39mfilter_jit\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_step\u001b[39m(ode_func, optim, opt_state):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m     loss, grad \u001b[39m=\u001b[39m loss_grad(ode_func)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m     updates, opt_state \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39;49mupdate(grad, opt_state)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m     ode_func \u001b[39m=\u001b[39m eqx\u001b[39m.\u001b[39mapply_updates(ode_func, updates)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#W4sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss, ode_func, opt_state\n",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/optax/_src/combine.py:59\u001b[0m, in \u001b[0;36mchain.<locals>.update_fn\u001b[0;34m(updates, state, params, **extra_args)\u001b[0m\n\u001b[1;32m     57\u001b[0m new_state \u001b[39m=\u001b[39m []\n\u001b[1;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m s, fn \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(state, update_fns):\n\u001b[0;32m---> 59\u001b[0m   updates, new_s \u001b[39m=\u001b[39m fn(updates, s, params, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_args)\n\u001b[1;32m     60\u001b[0m   new_state\u001b[39m.\u001b[39mappend(new_s)\n\u001b[1;32m     61\u001b[0m \u001b[39mreturn\u001b[39;00m updates, \u001b[39mtuple\u001b[39m(new_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/optax/_src/base.py:311\u001b[0m, in \u001b[0;36mwith_extra_args_support.<locals>.update\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(updates, state, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_args):\n\u001b[1;32m    310\u001b[0m   \u001b[39mdel\u001b[39;00m extra_args\n\u001b[0;32m--> 311\u001b[0m   \u001b[39mreturn\u001b[39;00m tx\u001b[39m.\u001b[39;49mupdate(updates, state, params)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/optax/_src/transform.py:768\u001b[0m, in \u001b[0;36madd_decayed_weights.<locals>.update_fn\u001b[0;34m(updates, state, params)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fn\u001b[39m(updates, state, params):\n\u001b[1;32m    767\u001b[0m   \u001b[39mif\u001b[39;00m params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(base\u001b[39m.\u001b[39mNO_PARAMS_MSG)\n\u001b[1;32m    769\u001b[0m   updates \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mtree_util\u001b[39m.\u001b[39mtree_map(\n\u001b[1;32m    770\u001b[0m       \u001b[39mlambda\u001b[39;00m g, p: g \u001b[39m+\u001b[39m weight_decay \u001b[39m*\u001b[39m p, updates, params)\n\u001b[1;32m    771\u001b[0m   \u001b[39mreturn\u001b[39;00m updates, state\n",
      "\u001b[0;31mValueError\u001b[0m: You are using a transformation that requires the current value of parameters, but you are not passing `params` when calling `update`."
     ]
    }
   ],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "mb_agent_jas = ODEAgent_jax(\n",
    "    env=env,\n",
    "    hidden_size=128,\n",
    "    num_layers=4,\n",
    "    ensemble_size=10,\n",
    "    train_timestep=0.005,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_timestep=0.005,\n",
    "    mpc_strategy=\"random\",\n",
    "    mpc_num_action_sequences=10,\n",
    "    key=key\n",
    ")\n",
    "replay_buffer = ReplayBufferTrajectories(seed=0)\n",
    "trajs, _ = sample_n_trajectories(env, RandomPolicy(env=env), ntraj=10, max_length=200)\n",
    "replay_buffer.add_rollouts(trajs)\n",
    "\n",
    "for n in trange(1000):\n",
    "    for i in range(mb_agent_jas.ensemble_size):\n",
    "        traj = replay_buffer.sample_rollout()\n",
    "        mb_agent_jas.update(i, traj[\"observations\"], traj[\"actions\"], jnp.cumsum(traj[\"dts\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(nn.Module):\n",
    "    _str_to_activation = {\n",
    "        \"relu\": nn.ReLU(),\n",
    "        \"tanh\": nn.Tanh(),\n",
    "        \"leaky_relu\": nn.LeakyReLU(),\n",
    "        \"sigmoid\": nn.Sigmoid(),\n",
    "        \"selu\": nn.SELU(),\n",
    "        \"softplus\": nn.Softplus(),\n",
    "        \"identity\": nn.Identity(),\n",
    "    }\n",
    "    def __init__(self, hidden_dims, ob_dim, ac_dim, activation=\"relu\", output_activation='identity'):\n",
    "        super().__init__()\n",
    "        self.ac_dim = ac_dim\n",
    "        self.ob_dim = ob_dim\n",
    "        activation = self._str_to_activation[activation]\n",
    "        output_activation = self._str_to_activation[output_activation]\n",
    "        layers = []\n",
    "        hidden_dims = [ob_dim + ac_dim] + hidden_dims\n",
    "        for n in range(len(hidden_dims) - 1):\n",
    "            layers.append(nn.Linear(hidden_dims[n], hidden_dims[n+1]))\n",
    "            layers.append(activation)\n",
    "        layers.append(nn.Linear(hidden_dims[-1], ob_dim))\n",
    "        layers.append(output_activation)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "    \n",
    "    def update_action(self, actions: torch.Tensor, times: torch.Tensor):\n",
    "        ep_len = actions.shape[0]\n",
    "        assert actions.shape == (ep_len, self.ac_dim) and times.shape == (ep_len,)\n",
    "        # times = times - times[0] # start with t=0\n",
    "        # right now, do not assume t0 = 0\n",
    "        self.register_buffer(\"times\", times)\n",
    "        self.register_buffer(\"actions\", actions)\n",
    "\n",
    "    def _get_action(self, t):\n",
    "        idx = torch.searchsorted(self.times, t, right=True) - 1\n",
    "        return self.actions[idx]\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        ac = self._get_action(t)\n",
    "        return self.net(torch.cat((y, ac), dim=-1))\n",
    "\n",
    "    \n",
    "class ODEAgent(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        hidden_dims: Sequence[int],\n",
    "        make_optimizer: Callable[[nn.ParameterList], torch.optim.Optimizer],\n",
    "        ensemble_size: int,\n",
    "        mpc_horizon_steps: int,\n",
    "        mpc_timestep: float,\n",
    "        mpc_strategy: str,\n",
    "        mpc_num_action_sequences: int,\n",
    "        cem_num_iters: Optional[int] = None,\n",
    "        cem_num_elites: Optional[int] = None,\n",
    "        cem_alpha: Optional[float] = None,\n",
    "        activation: str = \"relu\",\n",
    "        output_activation: str = \"identity\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.mpc_horizon_steps = mpc_horizon_steps # in terms of timesteps\n",
    "        self.mpc_strategy = mpc_strategy\n",
    "        self.mpc_num_action_sequences = mpc_num_action_sequences\n",
    "        self.cem_num_iters = cem_num_iters\n",
    "        self.cem_num_elites = cem_num_elites\n",
    "        self.cem_alpha = cem_alpha\n",
    "        self.mpc_timestep = mpc_timestep # when evaluating\n",
    "\n",
    "        assert mpc_strategy in (\n",
    "            \"random\",\n",
    "            \"cem\",\n",
    "        ), f\"'{mpc_strategy}' is not a valid MPC strategy\"\n",
    "\n",
    "        # ensure the environment is state-based\n",
    "        assert len(env.observation_space.shape) == 1\n",
    "        assert len(env.action_space.shape) == 1\n",
    "\n",
    "        self.ob_dim = env.observation_space.shape[0]\n",
    "        self.ac_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.ode_functions = nn.ModuleList(\n",
    "            [\n",
    "                NeuralODE(\n",
    "                    hidden_dims,\n",
    "                    self.ob_dim,\n",
    "                    self.ac_dim,\n",
    "                    activation,\n",
    "                    output_activation\n",
    "                ).to(ptu.device)\n",
    "                for _ in range(ensemble_size)\n",
    "            ]\n",
    "        )\n",
    "        self.optimizer = make_optimizer(self.ode_functions.parameters())\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def update(self, i: int, obs: np.ndarray, acs: np.ndarray, times: np.ndarray):\n",
    "        \"\"\"\n",
    "        Update self.dynamics_models[i] using the given trajectory\n",
    "\n",
    "        Args:\n",
    "            i: index of the dynamics model to update\n",
    "            obs: (ep_len, ob_dim)\n",
    "            acs: (ep_len, ac_dim)\n",
    "            times: (ep_len)\n",
    "        \"\"\"\n",
    "        obs = ptu.from_numpy(obs)\n",
    "        acs = ptu.from_numpy(acs)\n",
    "        times = ptu.from_numpy(times)\n",
    "        ode_func = self.ode_functions[i]\n",
    "        ode_func.update_action(acs, times)\n",
    "        ode_out = odeint(ode_func, obs[0, :], times) # t0 = times[0] in torchdiffeq\n",
    "        # possible problem: the ode function is only \"evaluating\" on times\n",
    "        # I am not sure whether there is an implicit dt or dt[i] = times[i+1] - times[i]\n",
    "        # I know for diffrax in jax, there is a separate dt argument passed into odeint()\n",
    "        assert ode_out.shape == obs.shape\n",
    "        loss = self.loss_fn(ode_out, obs)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return ptu.to_numpy(loss)\n",
    "    \n",
    "    def update_statistics(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_action_sequences(self, obs: np.ndarray, acs: np.ndarray):\n",
    "        obs = ptu.from_numpy(obs) # (ob_dim)\n",
    "        acs_np = acs\n",
    "        acs = ptu.from_numpy(acs) # (N, steps, ac_dim)\n",
    "        times = torch.linspace(0, (self.mpc_horizon_steps - 1) * self.mpc_timestep, self.mpc_horizon_steps, device=ptu.device)\n",
    "        reward_arr = np.zeros((self.mpc_num_action_sequences, self.ensemble_size))\n",
    "        for n in range(self.mpc_num_action_sequences):\n",
    "            for i in range(self.ensemble_size):\n",
    "                ode_func = self.ode_functions[i]\n",
    "                ode_func.update_action(acs[n, :, :], times)\n",
    "                ode_out = odeint(ode_func, obs, times) # (steps, ob_dim)\n",
    "                rewards, _ = self.env.get_reward(ptu.to_numpy(ode_out), acs_np[n, :, :])\n",
    "                avg_reward = np.mean(rewards)\n",
    "                reward_arr[n, i] = avg_reward\n",
    "        return np.mean(reward_arr, axis=1)\n",
    "    # maybe I should manually implement batched Euler solver\n",
    "    # to make inference faster\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, obs: np.ndarray):\n",
    "        \"\"\"\n",
    "        Choose the best action using model-predictive control.\n",
    "\n",
    "        Args:\n",
    "            obs: (ob_dim,)\n",
    "        \"\"\"\n",
    "        # always start with uniformly random actions\n",
    "        actions = np.random.uniform(\n",
    "            self.env.action_space.low,\n",
    "            self.env.action_space.high,\n",
    "            size=(self.mpc_num_action_sequences, self.mpc_horizon_steps, self.ac_dim),\n",
    "        )\n",
    "\n",
    "        if self.mpc_strategy == \"random\":\n",
    "            # evaluate each action sequence and return the best one\n",
    "            rewards = self.evaluate_action_sequences(obs, actions)\n",
    "            assert rewards.shape == (self.mpc_num_action_sequences,)\n",
    "            best_index = np.argmax(rewards)\n",
    "            return actions[best_index, 0, :]\n",
    "        elif self.mpc_strategy == \"cem\":\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid MPC strategy '{self.mpc_strategy}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "mb_agent = ODEAgent(\n",
    "    env=env,\n",
    "    hidden_dims=[128, 128, 128],\n",
    "    make_optimizer=lambda param_list: torch.optim.AdamW(param_list),\n",
    "    ensemble_size=10,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_timestep=0.005,\n",
    "    mpc_strategy=\"random\",\n",
    "    mpc_num_action_sequences=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.random.uniform(\n",
    "    mb_agent.env.action_space.low,\n",
    "    mb_agent.env.action_space.high,\n",
    "    size=(mb_agent.mpc_num_action_sequences, mb_agent.mpc_horizon_steps, mb_agent.ac_dim),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 82.8017 s\n",
      "File: /tmp/ipykernel_186605/3267144233.py\n",
      "Function: evaluate_action_sequences at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def evaluate_action_sequences(agent, obs: np.ndarray, acs: np.ndarray):\n",
      "     2         1      13825.0  13825.0      0.0      with torch.no_grad():\n",
      "     3         1      24795.0  24795.0      0.0          obs = ptu.from_numpy(obs)\n",
      "     4         1        116.0    116.0      0.0          acs_np = acs\n",
      "     5         1      83303.0  83303.0      0.0          acs = ptu.from_numpy(acs)\n",
      "     6         1      24536.0  24536.0      0.0          times = torch.linspace(0, (agent.mpc_horizon_steps - 1) * agent.mpc_timestep, agent.mpc_horizon_steps, device=ptu.device)\n",
      "     7         1       6850.0   6850.0      0.0          reward_arr = np.zeros((agent.mpc_num_action_sequences, agent.ensemble_size))\n",
      "     8        11       2248.0    204.4      0.0          for n in range(agent.mpc_num_action_sequences):\n",
      "     9       110      34870.0    317.0      0.0              for i in range(agent.ensemble_size):\n",
      "    10       100     832289.0   8322.9      0.0                  ode_func = agent.ode_functions[i]\n",
      "    11       100    1429678.0  14296.8      0.0                  ode_func.update_action(acs[n, :, :], times)\n",
      "    12       100        8e+10    8e+08    100.0                  ode_out = odeint(ode_func, obs, times) # (steps, ob_dim)\n",
      "    13       100    6478946.0  64789.5      0.0                  rewards, _ = agent.env.get_reward(ptu.to_numpy(ode_out), acs_np[n, :, :])\n",
      "    14       100    3020976.0  30209.8      0.0                  avg_reward = np.mean(rewards)\n",
      "    15       100      68506.0    685.1      0.0                  reward_arr[n, i] = avg_reward\n",
      "    16         1      26776.0  26776.0      0.0          return np.mean(reward_arr, axis=1)"
     ]
    }
   ],
   "source": [
    "%lprun -f evaluate_action_sequences evaluate_action_sequences(mb_agent, ob, actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
