{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler\n",
    "from cs285.envs.pendulum.pendulum_env import PendulumEnv\n",
    "from cs285.envs.dt_sampler import ConstantSampler\n",
    "from cs285.infrastructure.replay_buffer import ReplayBufferTrajectories\n",
    "from cs285.infrastructure.utils import sample_n_trajectories, RandomPolicy\n",
    "from typing import Callable, Optional, Tuple, Sequence\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import gym\n",
    "from cs285.infrastructure import pytorch_util as ptu\n",
    "from torchdiffeq import odeint\n",
    "from tqdm import trange\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "from diffrax import diffeqsolve, Dopri5\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = eqx.nn.MLP(in_size=3+1, out_size=3, width_size=32, depth=3, key=key)\n",
    "def f(t, y, args):\n",
    "    times = args[0] # args[\"times\"] # (ep_len,)\n",
    "    actions = args[1] # args[\"actions\"] # (ep_len, ac_dim)\n",
    "    idx = jnp.searchsorted(times, t, side=\"right\") - 1\n",
    "    action = actions[idx] # (ac_dim)\n",
    "    # althoug I believe this should also work for batched \n",
    "    return mlp(jnp.concatenate((y, action), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  layers=(\n",
       "    Linear(\n",
       "      weight=f32[32,4],\n",
       "      bias=f32[32],\n",
       "      in_features=4,\n",
       "      out_features=32,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    Linear(\n",
       "      weight=f32[32,32],\n",
       "      bias=f32[32],\n",
       "      in_features=32,\n",
       "      out_features=32,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    Linear(\n",
       "      weight=f32[32,32],\n",
       "      bias=f32[32],\n",
       "      in_features=32,\n",
       "      out_features=32,\n",
       "      use_bias=True\n",
       "    ),\n",
       "    Linear(\n",
       "      weight=f32[3,32],\n",
       "      bias=f32[3],\n",
       "      in_features=32,\n",
       "      out_features=3,\n",
       "      use_bias=True\n",
       "    )\n",
       "  ),\n",
       "  activation=<wrapped function relu>,\n",
       "  final_activation=<function <lambda>>,\n",
       "  use_bias=True,\n",
       "  use_final_bias=True,\n",
       "  in_size=4,\n",
       "  out_size=3,\n",
       "  width_size=32,\n",
       "  depth=3\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = jnp.linspace(0, 1, 100)\n",
    "actions = jax.random.normal(key=key, shape=(100,1))\n",
    "observations = jax.random.normal(key=key, shape=(100,3))\n",
    "args = [times, actions]\n",
    "solver = Dopri5()\n",
    "optim = optax.adamw(learning_rate=0.01)\n",
    "optim_state = optim.init(eqx.filter(mlp, eqx.is_array))\n",
    "\n",
    "@eqx.filter_jit\n",
    "@eqx.filter_value_and_grad\n",
    "def vag(mlp):\n",
    "    def f(t, y, args):\n",
    "        times = args[0] # args[\"times\"] # (ep_len,)\n",
    "        actions = args[1] # args[\"actions\"] # (ep_len, ac_dim)\n",
    "        idx = jnp.searchsorted(times, t, side=\"right\") - 1\n",
    "        action = actions[idx] # (ac_dim)\n",
    "        # althoug I believe this should also work for batched \n",
    "        return mlp(jnp.concatenate((y, action), axis=-1))\n",
    "    ode_out = diffeqsolve(terms=diffrax.ODETerm(f),\n",
    "                        solver=solver,\n",
    "                        t0=times[0],\n",
    "                        t1=times[-1],\n",
    "                        dt0=0.005,\n",
    "                        y0=observations[0],\n",
    "                        saveat=diffrax.SaveAt(ts=times),\n",
    "                        args=args)\n",
    "    return jnp.mean((ode_out.ys - observations) ** 2)\n",
    "\n",
    "value, grad = vag(mlp)\n",
    "updates, opt_state = optim.update(grad, optim_state, mlp)\n",
    "eqx.apply_updates(mlp, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE_jax(eqx.Module):\n",
    "    _str_to_activation = {\n",
    "        \"relu\": jax.nn.relu,\n",
    "        \"tanh\": jax.nn.tanh,\n",
    "        \"leaky_relu\": jax.nn.leaky_relu,\n",
    "        \"sigmoid\": jax.nn.sigmoid,\n",
    "        \"selu\": jax.nn.selu,\n",
    "        \"softplus\": jax.nn.softplus,\n",
    "        \"identity\": lambda x: x,\n",
    "    }\n",
    "    mlp: eqx.nn.MLP\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            ob_dim,\n",
    "            ac_dim,\n",
    "            key,\n",
    "            activation=\"relu\",\n",
    "            output_activation=\"identity\",\n",
    "        ):\n",
    "        super().__init__()\n",
    "        activation = self._str_to_activation[activation]\n",
    "        output_activation = self._str_to_activation[output_activation]\n",
    "        # hidden_size is an integer\n",
    "        self.mlp = eqx.nn.MLP(in_size=ob_dim+ac_dim,\n",
    "                              out_size=ob_dim,\n",
    "                              width_size=hidden_size,\n",
    "                              depth=num_layers,\n",
    "                              activation=activation,\n",
    "                              final_activation=output_activation,\n",
    "                              key=key)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, t, y, args):\n",
    "        # args is a dictionary that contains times and actions\n",
    "        times = args[\"times\"] # (ep_len,)\n",
    "        actions = args[\"actions\"] # (ep_len, ac_dim)\n",
    "        idx = jnp.searchsorted(times, t, side=\"right\") - 1\n",
    "        action = actions[idx] # (ac_dim)\n",
    "        # althoug I believe this should also work for batched\n",
    "        return self.mlp(jnp.concatenate((y, action), axis=-1))\n",
    "    \n",
    "class ODEAgent_jax():\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        key,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        ensemble_size: int,\n",
    "        train_timestep: float,\n",
    "        mpc_horizon_steps: int,\n",
    "        mpc_timestep: float,\n",
    "        mpc_strategy: str,\n",
    "        mpc_num_action_sequences: int,\n",
    "        cem_num_iters: Optional[int] = None,\n",
    "        cem_num_elites: Optional[int] = None,\n",
    "        cem_alpha: Optional[float] = None,\n",
    "        activation: str = \"relu\",\n",
    "        output_activation: str = \"identity\",\n",
    "        lr: float=0.001\n",
    "    ):\n",
    "        # super().__init__()\n",
    "        self.env = env\n",
    "        self.train_timestep = train_timestep\n",
    "        self.mpc_horizon_steps = mpc_horizon_steps # in terms of timesteps\n",
    "        self.mpc_strategy = mpc_strategy\n",
    "        self.mpc_num_action_sequences = mpc_num_action_sequences\n",
    "        self.cem_num_iters = cem_num_iters\n",
    "        self.cem_num_elites = cem_num_elites\n",
    "        self.cem_alpha = cem_alpha\n",
    "        self.mpc_timestep = mpc_timestep # when evaluating\n",
    "\n",
    "        assert mpc_strategy in (\n",
    "            \"random\",\n",
    "            \"cem\",\n",
    "        ), f\"'{mpc_strategy}' is not a valid MPC strategy\"\n",
    "\n",
    "        # ensure the environment is state-based\n",
    "        assert len(env.observation_space.shape) == 1\n",
    "        assert len(env.action_space.shape) == 1\n",
    "\n",
    "        self.ob_dim = env.observation_space.shape[0]\n",
    "        self.ac_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.ensemble_size = ensemble_size\n",
    "        keys = jax.random.split(key, ensemble_size)\n",
    "        self.ode_functions = [NeuralODE_jax(\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            ob_dim=self.ob_dim,\n",
    "            ac_dim=self.ac_dim,\n",
    "            activation=activation,\n",
    "            output_activation=output_activation,\n",
    "            key = keys[n]\n",
    "            ) for n in range(ensemble_size)]\n",
    "        self.optims = [optax.adamw(lr) for _ in range(ensemble_size)]\n",
    "        self.optim_states = [self.optims[n].init(eqx.filter(self.ode_functions[n], eqx.is_array)) for n in range(self.ensemble_size)]\n",
    "\n",
    "        self.solver = Dopri5()\n",
    "    \n",
    "    def update(self, i: int, obs: np.ndarray, acs: np.ndarray, times: np.ndarray):\n",
    "        \"\"\"\n",
    "        Update self.dynamics_models[i] using the given trajectory\n",
    "\n",
    "        Args:\n",
    "            i: index of the dynamics model to update\n",
    "            obs: (ep_len, ob_dim)\n",
    "            acs: (ep_len, ac_dim)\n",
    "            times: (ep_len)\n",
    "        \"\"\"\n",
    "        @eqx.filter_value_and_grad\n",
    "        def loss_grad(ode_func):\n",
    "            sol = diffeqsolve(\n",
    "                diffrax.ODETerm(ode_func), \n",
    "                self.solver, \n",
    "                t0=times[0], \n",
    "                t1=times[-1],\n",
    "                dt0=self.train_timestep,\n",
    "                y0 = obs[0, :],\n",
    "                args={\"times\": times, \"actions\": acs},\n",
    "                saveat=diffrax.SaveAt(ts=times)\n",
    "            )\n",
    "            assert sol.ys.shape == obs.shape\n",
    "            return jnp.mean((sol.ys - obs) ** 2) # do we want a  \"discount\"-like trick\n",
    "\n",
    "        @eqx.filter_jit\n",
    "        def make_step(ode_func, optim, opt_state):\n",
    "            loss, grad = loss_grad(ode_func)\n",
    "            updates, opt_state = optim.update(grad, opt_state, ode_func)\n",
    "            ode_func = eqx.apply_updates(ode_func, updates)\n",
    "            return loss, ode_func, opt_state\n",
    "        \n",
    "        ode_func, optim, opt_state = self.ode_functions[i], self.optims[i], self.optim_states[i]\n",
    "        loss, ode_func, opt_state = make_step(ode_func, optim, opt_state)\n",
    "        self.ode_functions[i], self.optim_states[i] = ode_func, opt_state\n",
    "        return loss.item()\n",
    "    \n",
    "\n",
    "    def evaluate_action_sequences(self, obs: jnp.ndarray, acs: jnp.ndarray):\n",
    "        times = jnp.linspace(0, (self.mpc_horizon_steps - 1) * self.mpc_timestep, self.mpc_horizon_steps)\n",
    "\n",
    "        @eqx.filter_jit\n",
    "        def evaluate_single_sequnce(ac):\n",
    "            avg_rewards = jnp.zeros((self.ensemble_size,))\n",
    "            for i in range(self.ensemble_size):\n",
    "                ode_func = self.ode_functions[i]\n",
    "                ode_out = diffeqsolve(\n",
    "                    terms=diffrax.ODETerm(ode_func),\n",
    "                    solver=self.solver,\n",
    "                    t0=times[0],\n",
    "                    t1=times[-1],\n",
    "                    dt0=self.mpc_timestep,\n",
    "                    y0=obs,\n",
    "                    args={\"times\": times, \"actions\": ac},\n",
    "                    saveat=diffrax.SaveAt(ts=times)\n",
    "                )\n",
    "                rewards, _ = self.env.get_reward_jnp(ode_out.ys, ac)\n",
    "                avg_rewards.at[i].set(jnp.mean(rewards))\n",
    "            return jnp.mean(avg_rewards)\n",
    "        \n",
    "        avg_rewards = jax.vmap(evaluate_single_sequnce)(acs) # (seqs,)\n",
    "        assert avg_rewards.shape == (self.mpc_num_action_sequences,)\n",
    "        return avg_rewards\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def get_action(self, obs: jnp.ndarray):\n",
    "        \"\"\"\n",
    "        Choose the best action using model-predictive control.\n",
    "\n",
    "        Args:\n",
    "            obs: (ob_dim,)\n",
    "        \"\"\"\n",
    "        # always start with uniformly random actions\n",
    "        actions = jax.random.uniform(\n",
    "            key=key,\n",
    "            minval=self.env.action_space.low,\n",
    "            maxval=self.env.action_space.high,\n",
    "            shape=(self.mpc_num_action_sequences, self.mpc_horizon_steps, self.ac_dim),\n",
    "        )\n",
    "\n",
    "        if self.mpc_strategy == \"random\":\n",
    "            # evaluate each action sequence and return the best one\n",
    "            rewards = self.evaluate_action_sequences(obs, actions)\n",
    "            assert rewards.shape == (self.mpc_num_action_sequences,)\n",
    "            best_index = jnp.argmax(rewards)\n",
    "            return actions[best_index, 0, :]\n",
    "        elif self.mpc_strategy == \"cem\":\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid MPC strategy '{self.mpc_strategy}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 56.21it/s]\n",
      "100%|██████████| 2/2 [01:49<00:00, 54.88s/it]\n"
     ]
    }
   ],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "mb_agent_jax = ODEAgent_jax(\n",
    "    env=env,\n",
    "    hidden_size=128,\n",
    "    num_layers=4,\n",
    "    ensemble_size=10,\n",
    "    train_timestep=0.005,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_timestep=0.005,\n",
    "    mpc_strategy=\"random\",\n",
    "    mpc_num_action_sequences=10,\n",
    "    key=key\n",
    ")\n",
    "replay_buffer = ReplayBufferTrajectories(seed=0)\n",
    "trajs, _ = sample_n_trajectories(env, RandomPolicy(env=env), ntraj=10, max_length=200)\n",
    "replay_buffer.add_rollouts(trajs)\n",
    "\n",
    "for n in trange(2):\n",
    "    for i in range(mb_agent_jax.ensemble_size):\n",
    "        traj = replay_buffer.sample_rollout()\n",
    "        mb_agent_jax.update(i, traj[\"observations\"], traj[\"actions\"], jnp.cumsum(traj[\"dts\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem: training is slower on a GPU compared to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = env.reset()\n",
    "ac = mb_agent_jax.get_action(ob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1.7681837], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "trajs = sample_n_trajectories(\n",
    "    env=env,\n",
    "    policy=mb_agent_jax,\n",
    "    ntraj=2,\n",
    "    max_length=200,\n",
    "    render=False\n",
    ") # this is fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(nn.Module):\n",
    "    _str_to_activation = {\n",
    "        \"relu\": nn.ReLU(),\n",
    "        \"tanh\": nn.Tanh(),\n",
    "        \"leaky_relu\": nn.LeakyReLU(),\n",
    "        \"sigmoid\": nn.Sigmoid(),\n",
    "        \"selu\": nn.SELU(),\n",
    "        \"softplus\": nn.Softplus(),\n",
    "        \"identity\": nn.Identity(),\n",
    "    }\n",
    "    def __init__(self, hidden_dims, ob_dim, ac_dim, activation=\"relu\", output_activation='identity'):\n",
    "        super().__init__()\n",
    "        self.ac_dim = ac_dim\n",
    "        self.ob_dim = ob_dim\n",
    "        activation = self._str_to_activation[activation]\n",
    "        output_activation = self._str_to_activation[output_activation]\n",
    "        layers = []\n",
    "        hidden_dims = [ob_dim + ac_dim] + hidden_dims\n",
    "        for n in range(len(hidden_dims) - 1):\n",
    "            layers.append(nn.Linear(hidden_dims[n], hidden_dims[n+1]))\n",
    "            layers.append(activation)\n",
    "        layers.append(nn.Linear(hidden_dims[-1], ob_dim))\n",
    "        layers.append(output_activation)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "    \n",
    "    def update_action(self, actions: torch.Tensor, times: torch.Tensor):\n",
    "        ep_len = actions.shape[0]\n",
    "        assert actions.shape == (ep_len, self.ac_dim) and times.shape == (ep_len,)\n",
    "        # times = times - times[0] # start with t=0\n",
    "        # right now, do not assume t0 = 0\n",
    "        self.register_buffer(\"times\", times)\n",
    "        self.register_buffer(\"actions\", actions)\n",
    "\n",
    "    def _get_action(self, t):\n",
    "        idx = torch.searchsorted(self.times, t, right=True) - 1\n",
    "        return self.actions[idx]\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        ac = self._get_action(t)\n",
    "        return self.net(torch.cat((y, ac), dim=-1))\n",
    "\n",
    "    \n",
    "class ODEAgent(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        hidden_dims: Sequence[int],\n",
    "        make_optimizer: Callable[[nn.ParameterList], torch.optim.Optimizer],\n",
    "        ensemble_size: int,\n",
    "        mpc_horizon_steps: int,\n",
    "        mpc_timestep: float,\n",
    "        mpc_strategy: str,\n",
    "        mpc_num_action_sequences: int,\n",
    "        cem_num_iters: Optional[int] = None,\n",
    "        cem_num_elites: Optional[int] = None,\n",
    "        cem_alpha: Optional[float] = None,\n",
    "        activation: str = \"relu\",\n",
    "        output_activation: str = \"identity\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.mpc_horizon_steps = mpc_horizon_steps # in terms of timesteps\n",
    "        self.mpc_strategy = mpc_strategy\n",
    "        self.mpc_num_action_sequences = mpc_num_action_sequences\n",
    "        self.cem_num_iters = cem_num_iters\n",
    "        self.cem_num_elites = cem_num_elites\n",
    "        self.cem_alpha = cem_alpha\n",
    "        self.mpc_timestep = mpc_timestep # when evaluating\n",
    "\n",
    "        assert mpc_strategy in (\n",
    "            \"random\",\n",
    "            \"cem\",\n",
    "        ), f\"'{mpc_strategy}' is not a valid MPC strategy\"\n",
    "\n",
    "        # ensure the environment is state-based\n",
    "        assert len(env.observation_space.shape) == 1\n",
    "        assert len(env.action_space.shape) == 1\n",
    "\n",
    "        self.ob_dim = env.observation_space.shape[0]\n",
    "        self.ac_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.ode_functions = nn.ModuleList(\n",
    "            [\n",
    "                NeuralODE(\n",
    "                    hidden_dims,\n",
    "                    self.ob_dim,\n",
    "                    self.ac_dim,\n",
    "                    activation,\n",
    "                    output_activation\n",
    "                ).to(ptu.device)\n",
    "                for _ in range(ensemble_size)\n",
    "            ]\n",
    "        )\n",
    "        self.optimizer = make_optimizer(self.ode_functions.parameters())\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def update(self, i: int, obs: np.ndarray, acs: np.ndarray, times: np.ndarray):\n",
    "        \"\"\"\n",
    "        Update self.dynamics_models[i] using the given trajectory\n",
    "\n",
    "        Args:\n",
    "            i: index of the dynamics model to update\n",
    "            obs: (ep_len, ob_dim)\n",
    "            acs: (ep_len, ac_dim)\n",
    "            times: (ep_len)\n",
    "        \"\"\"\n",
    "        obs = ptu.from_numpy(obs)\n",
    "        acs = ptu.from_numpy(acs)\n",
    "        times = ptu.from_numpy(times)\n",
    "        ode_func = self.ode_functions[i]\n",
    "        ode_func.update_action(acs, times)\n",
    "        ode_out = odeint(ode_func, obs[0, :], times) # t0 = times[0] in torchdiffeq\n",
    "        # possible problem: the ode function is only \"evaluating\" on times\n",
    "        # I am not sure whether there is an implicit dt or dt[i] = times[i+1] - times[i]\n",
    "        # I know for diffrax in jax, there is a separate dt argument passed into odeint()\n",
    "        assert ode_out.shape == obs.shape\n",
    "        loss = self.loss_fn(ode_out, obs)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return ptu.to_numpy(loss)\n",
    "    \n",
    "    def update_statistics(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_action_sequences(self, obs: np.ndarray, acs: np.ndarray):\n",
    "        obs = ptu.from_numpy(obs) # (ob_dim)\n",
    "        acs_np = acs\n",
    "        acs = ptu.from_numpy(acs) # (N, steps, ac_dim)\n",
    "        times = torch.linspace(0, (self.mpc_horizon_steps - 1) * self.mpc_timestep, self.mpc_horizon_steps, device=ptu.device)\n",
    "        reward_arr = np.zeros((self.mpc_num_action_sequences, self.ensemble_size))\n",
    "        for n in range(self.mpc_num_action_sequences):\n",
    "            for i in range(self.ensemble_size):\n",
    "                ode_func = self.ode_functions[i]\n",
    "                ode_func.update_action(acs[n, :, :], times)\n",
    "                ode_out = odeint(ode_func, obs, times) # (steps, ob_dim)\n",
    "                rewards, _ = self.env.get_reward(ptu.to_numpy(ode_out), acs_np[n, :, :])\n",
    "                avg_reward = np.mean(rewards)\n",
    "                reward_arr[n, i] = avg_reward\n",
    "        return np.mean(reward_arr, axis=1)\n",
    "    # maybe I should manually implement batched Euler solver\n",
    "    # to make inference faster\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, obs: np.ndarray):\n",
    "        \"\"\"\n",
    "        Choose the best action using model-predictive control.\n",
    "\n",
    "        Args:\n",
    "            obs: (ob_dim,)\n",
    "        \"\"\"\n",
    "        # always start with uniformly random actions\n",
    "        actions = np.random.uniform(\n",
    "            self.env.action_space.low,\n",
    "            self.env.action_space.high,\n",
    "            size=(self.mpc_num_action_sequences, self.mpc_horizon_steps, self.ac_dim),\n",
    "        )\n",
    "\n",
    "        if self.mpc_strategy == \"random\":\n",
    "            # evaluate each action sequence and return the best one\n",
    "            rewards = self.evaluate_action_sequences(obs, actions)\n",
    "            assert rewards.shape == (self.mpc_num_action_sequences,)\n",
    "            best_index = np.argmax(rewards)\n",
    "            return actions[best_index, 0, :]\n",
    "        elif self.mpc_strategy == \"cem\":\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid MPC strategy '{self.mpc_strategy}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 49.64it/s]\n",
      "  0%|          | 3/1000 [01:04<5:54:40, 21.34s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/zekai.wang/Desktop/College/Berkeley_Course/2023FA/CS 285/project/cs285_proj/notebooks/inference_speed.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zekai.wang/Desktop/College/Berkeley_Course/2023FA/CS%20285/project/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(mb_agent_jas\u001b[39m.\u001b[39mensemble_size):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zekai.wang/Desktop/College/Berkeley_Course/2023FA/CS%20285/project/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     traj \u001b[39m=\u001b[39m replay_buffer\u001b[39m.\u001b[39msample_rollout()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zekai.wang/Desktop/College/Berkeley_Course/2023FA/CS%20285/project/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     mb_agent_jas\u001b[39m.\u001b[39;49mupdate(i, traj[\u001b[39m\"\u001b[39;49m\u001b[39mobservations\u001b[39;49m\u001b[39m\"\u001b[39;49m], traj[\u001b[39m\"\u001b[39;49m\u001b[39mactions\u001b[39;49m\u001b[39m\"\u001b[39;49m], jnp\u001b[39m.\u001b[39;49mcumsum(traj[\u001b[39m\"\u001b[39;49m\u001b[39mdts\u001b[39;49m\u001b[39m\"\u001b[39;49m]))\n",
      "\u001b[1;32m/Users/zekai.wang/Desktop/College/Berkeley_Course/2023FA/CS 285/project/cs285_proj/notebooks/inference_speed.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/zekai.wang/Desktop/College/Berkeley_Course/2023FA/CS%20285/project/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss, ode_func, opt_state\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/zekai.wang/Desktop/College/Berkeley_Course/2023FA/CS%20285/project/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m ode_func, optim, opt_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mode_functions[i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptims[i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_states[i]\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/zekai.wang/Desktop/College/Berkeley_Course/2023FA/CS%20285/project/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m loss, ode_func, opt_state \u001b[39m=\u001b[39m make_step(ode_func, optim, opt_state)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/zekai.wang/Desktop/College/Berkeley_Course/2023FA/CS%20285/project/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mode_functions[i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_states[i] \u001b[39m=\u001b[39m ode_func, opt_state\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/zekai.wang/Desktop/College/Berkeley_Course/2023FA/CS%20285/project/cs285_proj/notebooks/inference_speed.ipynb#X21sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mitem()\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/pjit.py:255\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39m@api_boundary\u001b[39m\n\u001b[1;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcache_miss\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 255\u001b[0m   outs, out_flat, out_tree, args_flat, jaxpr \u001b[39m=\u001b[39m _python_pjit_helper(\n\u001b[1;32m    256\u001b[0m       fun, infer_params_fn, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    257\u001b[0m   executable \u001b[39m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[1;32m    258\u001b[0m   fastpath_data \u001b[39m=\u001b[39m _get_fastpath_data(executable, out_tree, args_flat, out_flat)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/pjit.py:166\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, infer_params_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m   dispatch\u001b[39m.\u001b[39mcheck_arg(arg)\n\u001b[1;32m    165\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m   out_flat \u001b[39m=\u001b[39m pjit_p\u001b[39m.\u001b[39;49mbind(\u001b[39m*\u001b[39;49margs_flat, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    167\u001b[0m \u001b[39mexcept\u001b[39;00m pxla\u001b[39m.\u001b[39mDeviceAssignmentMismatchError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    168\u001b[0m   fails, \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39margs\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/core.py:2682\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2678\u001b[0m axis_main \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m((axis_frame(a)\u001b[39m.\u001b[39mmain_trace \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m used_axis_names(\u001b[39mself\u001b[39m, params)),\n\u001b[1;32m   2679\u001b[0m                 default\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m t: \u001b[39mgetattr\u001b[39m(t, \u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m   2680\u001b[0m top_trace \u001b[39m=\u001b[39m (top_trace \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m axis_main \u001b[39mor\u001b[39;00m axis_main\u001b[39m.\u001b[39mlevel \u001b[39m<\u001b[39m top_trace\u001b[39m.\u001b[39mlevel\n\u001b[1;32m   2681\u001b[0m              \u001b[39melse\u001b[39;00m axis_main\u001b[39m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2682\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(top_trace, args, params)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/core.py:405\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 405\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[1;32m    406\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/core.py:893\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 893\u001b[0m   \u001b[39mreturn\u001b[39;00m primitive\u001b[39m.\u001b[39;49mimpl(\u001b[39m*\u001b[39;49mtracers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/pjit.py:1238\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1235\u001b[0m donated_argnums \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i, d \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(donated_invars) \u001b[39mif\u001b[39;00m d]\n\u001b[1;32m   1236\u001b[0m has_explicit_sharding \u001b[39m=\u001b[39m _pjit_explicit_sharding(\n\u001b[1;32m   1237\u001b[0m     in_shardings, out_shardings, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1238\u001b[0m \u001b[39mreturn\u001b[39;00m xc\u001b[39m.\u001b[39;49m_xla\u001b[39m.\u001b[39;49mpjit(name, f, call_impl_cache_miss, [], [], donated_argnums,\n\u001b[1;32m   1239\u001b[0m                     tree_util\u001b[39m.\u001b[39;49mdispatch_registry,\n\u001b[1;32m   1240\u001b[0m                     _get_cpp_global_cache(has_explicit_sharding))(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/pjit.py:1222\u001b[0m, in \u001b[0;36m_pjit_call_impl.<locals>.call_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_impl_cache_miss\u001b[39m(\u001b[39m*\u001b[39margs_, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_):\n\u001b[0;32m-> 1222\u001b[0m   out_flat, compiled \u001b[39m=\u001b[39m _pjit_call_impl_python(\n\u001b[1;32m   1223\u001b[0m       \u001b[39m*\u001b[39;49margs, jaxpr\u001b[39m=\u001b[39;49mjaxpr, in_shardings\u001b[39m=\u001b[39;49min_shardings,\n\u001b[1;32m   1224\u001b[0m       out_shardings\u001b[39m=\u001b[39;49mout_shardings, resource_env\u001b[39m=\u001b[39;49mresource_env,\n\u001b[1;32m   1225\u001b[0m       donated_invars\u001b[39m=\u001b[39;49mdonated_invars, name\u001b[39m=\u001b[39;49mname, keep_unused\u001b[39m=\u001b[39;49mkeep_unused,\n\u001b[1;32m   1226\u001b[0m       inline\u001b[39m=\u001b[39;49minline)\n\u001b[1;32m   1227\u001b[0m   fastpath_data \u001b[39m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m   1228\u001b[0m       compiled, tree_structure(out_flat), args, out_flat)\n\u001b[1;32m   1229\u001b[0m   \u001b[39mreturn\u001b[39;00m out_flat, fastpath_data\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/pjit.py:1155\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[39mglobal\u001b[39;00m _most_recent_pjit_call_executable\n\u001b[1;32m   1151\u001b[0m in_shardings \u001b[39m=\u001b[39m _resolve_in_shardings(\n\u001b[1;32m   1152\u001b[0m     args, in_shardings, out_shardings,\n\u001b[1;32m   1153\u001b[0m     resource_env\u001b[39m.\u001b[39mphysical_mesh \u001b[39mif\u001b[39;00m resource_env \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1155\u001b[0m compiled \u001b[39m=\u001b[39m _pjit_lower(\n\u001b[1;32m   1156\u001b[0m     jaxpr, in_shardings, out_shardings, resource_env,\n\u001b[1;32m   1157\u001b[0m     donated_invars, name, keep_unused, inline,\n\u001b[1;32m   1158\u001b[0m     lowering_parameters\u001b[39m=\u001b[39;49mmlir\u001b[39m.\u001b[39;49mLoweringParameters())\u001b[39m.\u001b[39;49mcompile()\n\u001b[1;32m   1159\u001b[0m _most_recent_pjit_call_executable\u001b[39m.\u001b[39mweak_key_dict[jaxpr] \u001b[39m=\u001b[39m compiled\n\u001b[1;32m   1160\u001b[0m \u001b[39m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py:2208\u001b[0m, in \u001b[0;36mMeshComputation.compile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompile\u001b[39m(\u001b[39mself\u001b[39m, compiler_options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m MeshExecutable:\n\u001b[1;32m   2207\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executable \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m compiler_options \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2208\u001b[0m     executable \u001b[39m=\u001b[39m UnloadedMeshExecutable\u001b[39m.\u001b[39;49mfrom_hlo(\n\u001b[1;32m   2209\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_hlo, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_args,\n\u001b[1;32m   2210\u001b[0m         compiler_options\u001b[39m=\u001b[39;49mcompiler_options)\n\u001b[1;32m   2211\u001b[0m     \u001b[39mif\u001b[39;00m compiler_options \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2212\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executable \u001b[39m=\u001b[39m executable\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py:2648\u001b[0m, in \u001b[0;36mUnloadedMeshExecutable.from_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2645\u001b[0m       mesh \u001b[39m=\u001b[39m i\u001b[39m.\u001b[39mmesh  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   2646\u001b[0m       \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 2648\u001b[0m xla_executable, compile_options \u001b[39m=\u001b[39m _cached_compilation(\n\u001b[1;32m   2649\u001b[0m     hlo, name, mesh, spmd_lowering,\n\u001b[1;32m   2650\u001b[0m     tuple_args, auto_spmd_lowering, allow_prop_to_outputs,\n\u001b[1;32m   2651\u001b[0m     \u001b[39mtuple\u001b[39;49m(host_callbacks), backend, da, pmap_nreps,\n\u001b[1;32m   2652\u001b[0m     compiler_options_keys, compiler_options_values)\n\u001b[1;32m   2654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(backend, \u001b[39m\"\u001b[39m\u001b[39mcompile_replicated\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2655\u001b[0m   semantics_in_shardings \u001b[39m=\u001b[39m SemanticallyEqualShardings(in_shardings)  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py:2517\u001b[0m, in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, _allow_propagation_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values)\u001b[0m\n\u001b[1;32m   2512\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, compile_options\n\u001b[1;32m   2514\u001b[0m \u001b[39mwith\u001b[39;00m dispatch\u001b[39m.\u001b[39mlog_elapsed_time(\n\u001b[1;32m   2515\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFinished XLA compilation of \u001b[39m\u001b[39m{fun_name}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m{elapsed_time}\u001b[39;00m\u001b[39m sec\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2516\u001b[0m     fun_name\u001b[39m=\u001b[39mname, event\u001b[39m=\u001b[39mdispatch\u001b[39m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2517\u001b[0m   xla_executable \u001b[39m=\u001b[39m compiler\u001b[39m.\u001b[39;49mcompile_or_get_cached(\n\u001b[1;32m   2518\u001b[0m       backend, computation, dev, compile_options, host_callbacks)\n\u001b[1;32m   2519\u001b[0m \u001b[39mreturn\u001b[39;00m xla_executable, compile_options\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/compiler.py:301\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks)\u001b[0m\n\u001b[1;32m    297\u001b[0m use_compilation_cache \u001b[39m=\u001b[39m (compilation_cache\u001b[39m.\u001b[39mis_initialized() \u001b[39mand\u001b[39;00m\n\u001b[1;32m    298\u001b[0m                          backend\u001b[39m.\u001b[39mplatform \u001b[39min\u001b[39;00m supported_platforms)\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_compilation_cache:\n\u001b[0;32m--> 301\u001b[0m   \u001b[39mreturn\u001b[39;00m backend_compile(backend, computation, compile_options,\n\u001b[1;32m    302\u001b[0m                          host_callbacks)\n\u001b[1;32m    304\u001b[0m \u001b[39mglobal\u001b[39;00m _cache_used\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _cache_used:\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/profiler.py:334\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    333\u001b[0m   \u001b[39mwith\u001b[39;00m TraceAnnotation(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    335\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/anaconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/compiler.py:256\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39m# we use a separate function call to ensure that XLA compilation appears\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39m# separately in Python profiling results\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39mif\u001b[39;00m host_callbacks:\n\u001b[0;32m--> 256\u001b[0m   \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mcompile(built_c, compile_options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    257\u001b[0m                          host_callbacks\u001b[39m=\u001b[39;49mhost_callbacks)\n\u001b[1;32m    258\u001b[0m \u001b[39m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39m# to take in `host_callbacks`\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mcompile(built_c, compile_options\u001b[39m=\u001b[39moptions)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "mb_agent = ODEAgent(\n",
    "    env=env,\n",
    "    hidden_dims=[128, 128, 128],\n",
    "    make_optimizer=lambda param_list: torch.optim.AdamW(param_list),\n",
    "    ensemble_size=10,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_timestep=0.005,\n",
    "    mpc_strategy=\"random\",\n",
    "    mpc_num_action_sequences=10,\n",
    ")\n",
    "replay_buffer = ReplayBufferTrajectories(seed=0)\n",
    "trajs, _ = sample_n_trajectories(env, RandomPolicy(env=env), ntraj=10, max_length=200)\n",
    "replay_buffer.add_rollouts(trajs)\n",
    "\n",
    "for n in trange(1000):\n",
    "    for i in range(mb_agent_jas.ensemble_size):\n",
    "        traj = replay_buffer.sample_rollout()\n",
    "        mb_agent_jas.update(i, traj[\"observations\"], traj[\"actions\"], jnp.cumsum(traj[\"dts\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "mb_agent = ODEAgent(\n",
    "    env=env,\n",
    "    hidden_dims=[128, 128, 128],\n",
    "    make_optimizer=lambda param_list: torch.optim.AdamW(param_list),\n",
    "    ensemble_size=10,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_timestep=0.005,\n",
    "    mpc_strategy=\"random\",\n",
    "    mpc_num_action_sequences=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.random.uniform(\n",
    "    mb_agent.env.action_space.low,\n",
    "    mb_agent.env.action_space.high,\n",
    "    size=(mb_agent.mpc_num_action_sequences, mb_agent.mpc_horizon_steps, mb_agent.ac_dim),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 82.8017 s\n",
      "File: /tmp/ipykernel_186605/3267144233.py\n",
      "Function: evaluate_action_sequences at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def evaluate_action_sequences(agent, obs: np.ndarray, acs: np.ndarray):\n",
      "     2         1      13825.0  13825.0      0.0      with torch.no_grad():\n",
      "     3         1      24795.0  24795.0      0.0          obs = ptu.from_numpy(obs)\n",
      "     4         1        116.0    116.0      0.0          acs_np = acs\n",
      "     5         1      83303.0  83303.0      0.0          acs = ptu.from_numpy(acs)\n",
      "     6         1      24536.0  24536.0      0.0          times = torch.linspace(0, (agent.mpc_horizon_steps - 1) * agent.mpc_timestep, agent.mpc_horizon_steps, device=ptu.device)\n",
      "     7         1       6850.0   6850.0      0.0          reward_arr = np.zeros((agent.mpc_num_action_sequences, agent.ensemble_size))\n",
      "     8        11       2248.0    204.4      0.0          for n in range(agent.mpc_num_action_sequences):\n",
      "     9       110      34870.0    317.0      0.0              for i in range(agent.ensemble_size):\n",
      "    10       100     832289.0   8322.9      0.0                  ode_func = agent.ode_functions[i]\n",
      "    11       100    1429678.0  14296.8      0.0                  ode_func.update_action(acs[n, :, :], times)\n",
      "    12       100        8e+10    8e+08    100.0                  ode_out = odeint(ode_func, obs, times) # (steps, ob_dim)\n",
      "    13       100    6478946.0  64789.5      0.0                  rewards, _ = agent.env.get_reward(ptu.to_numpy(ode_out), acs_np[n, :, :])\n",
      "    14       100    3020976.0  30209.8      0.0                  avg_reward = np.mean(rewards)\n",
      "    15       100      68506.0    685.1      0.0                  reward_arr[n, i] = avg_reward\n",
      "    16         1      26776.0  26776.0      0.0          return np.mean(reward_arr, axis=1)"
     ]
    }
   ],
   "source": [
    "%lprun -f evaluate_action_sequences evaluate_action_sequences(mb_agent, ob, actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
