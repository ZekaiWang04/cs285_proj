{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler\n",
    "from cs285.envs.pendulum.pendulum_env import PendulumEnv\n",
    "from cs285.envs.dt_sampler import ConstantSampler\n",
    "from cs285.infrastructure.replay_buffer import ReplayBufferTrajectories\n",
    "from cs285.infrastructure.utils import sample_n_trajectories, RandomPolicy\n",
    "from typing import Callable, Optional, Tuple, Sequence\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import gym\n",
    "from cs285.infrastructure import pytorch_util as ptu\n",
    "from torchdiffeq import odeint\n",
    "from tqdm import trange\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "from diffrax import diffeqsolve, Dopri5\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE_jax(eqx.Module):\n",
    "    _str_to_activation = {\n",
    "        \"relu\": jax.nn.relu,\n",
    "        \"tanh\": jax.nn.tanh,\n",
    "        \"leaky_relu\": jax.nn.leaky_relu,\n",
    "        \"sigmoid\": jax.nn.sigmoid,\n",
    "        \"selu\": jax.nn.selu,\n",
    "        \"softplus\": jax.nn.softplus,\n",
    "        \"identity\": lambda x: x,\n",
    "    }\n",
    "    mlp: eqx.nn.MLP\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            ob_dim,\n",
    "            ac_dim,\n",
    "            key,\n",
    "            activation=\"relu\",\n",
    "            output_activation=\"identity\",\n",
    "        ):\n",
    "        super().__init__()\n",
    "        activation = self._str_to_activation[activation]\n",
    "        output_activation = self._str_to_activation[output_activation]\n",
    "        # hidden_size is an integer\n",
    "        self.mlp = eqx.nn.MLP(in_size=ob_dim+ac_dim,\n",
    "                              out_size=ob_dim,\n",
    "                              width_size=hidden_size,\n",
    "                              depth=num_layers,\n",
    "                              activation=activation,\n",
    "                              final_activation=output_activation,\n",
    "                              key=key)\n",
    "\n",
    "    @jax.jit\n",
    "    def __call__(self, t, y, args):\n",
    "        # args is a dictionary that contains times and actions\n",
    "        times = args[\"times\"] # (ep_len,)\n",
    "        actions = args[\"actions\"] # (ep_len, ac_dim)\n",
    "        idx = jnp.searchsorted(times, t, side=\"right\") - 1\n",
    "        action = actions[idx] # (ac_dim)\n",
    "        # althoug I believe this should also work for batched \n",
    "        return self.mlp(jnp.concatenate((y, action), axis=-1))\n",
    "    \n",
    "class ODEAgent_jax():\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        key,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        ensemble_size: int,\n",
    "        train_timestep: float,\n",
    "        mpc_horizon_steps: int,\n",
    "        mpc_timestep: float,\n",
    "        mpc_strategy: str,\n",
    "        mpc_num_action_sequences: int,\n",
    "        cem_num_iters: Optional[int] = None,\n",
    "        cem_num_elites: Optional[int] = None,\n",
    "        cem_alpha: Optional[float] = None,\n",
    "        activation: str = \"relu\",\n",
    "        output_activation: str = \"identity\",\n",
    "        lr: float=0.001\n",
    "    ):\n",
    "        # super().__init__()\n",
    "        self.env = env\n",
    "        self.train_timestep = train_timestep\n",
    "        self.mpc_horizon_steps = mpc_horizon_steps # in terms of timesteps\n",
    "        self.mpc_strategy = mpc_strategy\n",
    "        self.mpc_num_action_sequences = mpc_num_action_sequences\n",
    "        self.cem_num_iters = cem_num_iters\n",
    "        self.cem_num_elites = cem_num_elites\n",
    "        self.cem_alpha = cem_alpha\n",
    "        self.mpc_timestep = mpc_timestep # when evaluating\n",
    "\n",
    "        assert mpc_strategy in (\n",
    "            \"random\",\n",
    "            \"cem\",\n",
    "        ), f\"'{mpc_strategy}' is not a valid MPC strategy\"\n",
    "\n",
    "        # ensure the environment is state-based\n",
    "        assert len(env.observation_space.shape) == 1\n",
    "        assert len(env.action_space.shape) == 1\n",
    "\n",
    "        self.ob_dim = env.observation_space.shape[0]\n",
    "        self.ac_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.ensemble_size = ensemble_size\n",
    "        keys = jax.random.split(key, ensemble_size)\n",
    "        self.ode_functions = [NeuralODE_jax(\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            ob_dim=self.ob_dim,\n",
    "            ac_dim=self.ac_dim,\n",
    "            activation=activation,\n",
    "            output_activation=output_activation,\n",
    "            key = keys[n]\n",
    "            ) for n in range(ensemble_size)]\n",
    "        self.optims = [optax.adamw(lr) for _ in range(ensemble_size)]\n",
    "        self.optim_states = [self.optims[n].init(eqx.filter(self.ode_functions[n].mlp, eqx.is_array)) for n in range(self.ensemble_size)]\n",
    "\n",
    "        self.solver = Dopri5()\n",
    "    \n",
    "    def update(self, i: int, obs: np.ndarray, acs: np.ndarray, times: np.ndarray):\n",
    "        \"\"\"\n",
    "        Update self.dynamics_models[i] using the given trajectory\n",
    "\n",
    "        Args:\n",
    "            i: index of the dynamics model to update\n",
    "            obs: (ep_len, ob_dim)\n",
    "            acs: (ep_len, ac_dim)\n",
    "            times: (ep_len)\n",
    "        \"\"\"\n",
    "        @eqx.filter_value_and_grad\n",
    "        def loss_grad(ode_func):\n",
    "            sol = diffeqsolve(\n",
    "                diffrax.ODETerm(ode_func), \n",
    "                self.solver, \n",
    "                t0=times[0], \n",
    "                t1=times[-1], \n",
    "                dt0=self.train_timestep,\n",
    "                y0 = obs[0, :],\n",
    "                args={\"times\": times, \"actions\": acs},\n",
    "                saveat=diffrax.SaveAt(times)\n",
    "            )\n",
    "            assert sol.ys.shape == obs.shape\n",
    "            return jnp.mean((sol.ys - obs) ** 2) # do we want a  \"discount\"-like trick\n",
    "\n",
    "        @eqx.filter_jit\n",
    "        def make_step(ode_func, optim, opt_state):\n",
    "            loss, grad = loss_grad(ode_func)\n",
    "            updates, opt_state = optim.update(grad, opt_state)\n",
    "            ode_func = eqx.apply_updates(ode_func, updates)\n",
    "            return loss, ode_func, opt_state\n",
    "        \n",
    "        ode_func, optim, opt_state = self.ode_functions[i], self.optims[i], self.optim_states[i]\n",
    "        loss, ode_func, opt_state = make_step(ode_func, optim, opt_state)\n",
    "        self.ode_functions[i], self.optim_states[i] = ode_func, opt_state\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 56.88it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "DynamicJaxprTracer has no attribute subs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/core.py:769\u001b[0m, in \u001b[0;36mTracer.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m   attr \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maval, name)\n\u001b[1;32m    770\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ShapedArray' object has no attribute 'subs'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(mb_agent_jas\u001b[39m.\u001b[39mensemble_size):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     traj \u001b[39m=\u001b[39m replay_buffer\u001b[39m.\u001b[39msample_rollout()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     mb_agent_jas\u001b[39m.\u001b[39;49mupdate(i, traj[\u001b[39m\"\u001b[39;49m\u001b[39mobservations\u001b[39;49m\u001b[39m\"\u001b[39;49m], traj[\u001b[39m\"\u001b[39;49m\u001b[39mactions\u001b[39;49m\u001b[39m\"\u001b[39;49m], jnp\u001b[39m.\u001b[39;49mcumsum(traj[\u001b[39m\"\u001b[39;49m\u001b[39mdts\u001b[39;49m\u001b[39m\"\u001b[39;49m]))\n",
      "\u001b[1;32m/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss, ode_func, opt_state\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m ode_func, optim, opt_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mode_functions[i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptims[i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_states[i]\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m loss, ode_func, opt_state \u001b[39m=\u001b[39m make_step(ode_func, optim, opt_state)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mode_functions[i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_states[i] \u001b[39m=\u001b[39m ode_func, opt_state\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;32m/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_step\u001b[39m(ode_func, optim, opt_state):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m     loss, grad \u001b[39m=\u001b[39m loss_grad(ode_func)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m     updates, opt_state \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mupdate(grad, opt_state)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m     ode_func \u001b[39m=\u001b[39m eqx\u001b[39m.\u001b[39mapply_updates(ode_func, updates)\n",
      "\u001b[1;32m/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_grad\u001b[39m(ode_func):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     sol \u001b[39m=\u001b[39m diffeqsolve(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m         diffrax\u001b[39m.\u001b[39;49mODETerm(ode_func), \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msolver, \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m         t0\u001b[39m=\u001b[39;49mtimes[\u001b[39m0\u001b[39;49m], \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m         t1\u001b[39m=\u001b[39;49mtimes[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m         dt0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_timestep,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m         y0 \u001b[39m=\u001b[39;49m obs[\u001b[39m0\u001b[39;49m, :],\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m         args\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtimes\u001b[39;49m\u001b[39m\"\u001b[39;49m: times, \u001b[39m\"\u001b[39;49m\u001b[39mactions\u001b[39;49m\u001b[39m\"\u001b[39;49m: acs},\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m         saveat\u001b[39m=\u001b[39;49mtimes\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m     \u001b[39massert\u001b[39;00m sol\u001b[39m.\u001b[39mys\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m obs\u001b[39m.\u001b[39mshape\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/zekai/repos/cs285_proj/notebooks/inference_speed.ipynb#X35sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mmean((sol\u001b[39m.\u001b[39mys \u001b[39m-\u001b[39m obs) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/diffrax/integrate.py:660\u001b[0m, in \u001b[0;36mdiffeqsolve\u001b[0;34m(terms, solver, t0, t1, dt0, y0, args, saveat, stepsize_controller, adjoint, discrete_terminating_event, max_steps, throw, solver_state, controller_state, made_jump)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    655\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`UnsafeBrownianPath` cannot be used with adaptive step sizes.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    656\u001b[0m         )\n\u001b[1;32m    658\u001b[0m \u001b[39m# Allow setting e.g. t0 as an int with dt0 as a float.\u001b[39;00m\n\u001b[1;32m    659\u001b[0m timelikes \u001b[39m=\u001b[39m [jnp\u001b[39m.\u001b[39marray(\u001b[39m0.0\u001b[39m), t0, t1, dt0] \u001b[39m+\u001b[39m [\n\u001b[0;32m--> 660\u001b[0m     s\u001b[39m.\u001b[39mts \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m jtu\u001b[39m.\u001b[39mtree_leaves(saveat\u001b[39m.\u001b[39;49msubs, is_leaf\u001b[39m=\u001b[39m_is_subsaveat)\n\u001b[1;32m    661\u001b[0m ]\n\u001b[1;32m    662\u001b[0m timelikes \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m timelikes \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    663\u001b[0m dtype \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mresult_type(\u001b[39m*\u001b[39mtimelikes)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs285_proj/lib/python3.9/site-packages/jax/_src/core.py:771\u001b[0m, in \u001b[0;36mTracer.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    769\u001b[0m   attr \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maval, name)\n\u001b[1;32m    770\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 771\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    772\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    773\u001b[0m   ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    775\u001b[0m   t \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(attr)\n",
      "\u001b[0;31mAttributeError\u001b[0m: DynamicJaxprTracer has no attribute subs"
     ]
    }
   ],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "mb_agent_jas = ODEAgent_jax(\n",
    "    env=env,\n",
    "    hidden_size=128,\n",
    "    num_layers=4,\n",
    "    ensemble_size=10,\n",
    "    train_timestep=0.005,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_timestep=0.005,\n",
    "    mpc_strategy=\"random\",\n",
    "    mpc_num_action_sequences=10,\n",
    "    key=key\n",
    ")\n",
    "replay_buffer = ReplayBufferTrajectories(seed=0)\n",
    "trajs, _ = sample_n_trajectories(env, RandomPolicy(env=env), ntraj=10, max_length=200)\n",
    "replay_buffer.add_rollouts(trajs)\n",
    "\n",
    "for n in trange(1000):\n",
    "    for i in range(mb_agent_jas.ensemble_size):\n",
    "        traj = replay_buffer.sample_rollout()\n",
    "        mb_agent_jas.update(i, traj[\"observations\"], traj[\"actions\"], jnp.cumsum(traj[\"dts\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(nn.Module):\n",
    "    _str_to_activation = {\n",
    "        \"relu\": nn.ReLU(),\n",
    "        \"tanh\": nn.Tanh(),\n",
    "        \"leaky_relu\": nn.LeakyReLU(),\n",
    "        \"sigmoid\": nn.Sigmoid(),\n",
    "        \"selu\": nn.SELU(),\n",
    "        \"softplus\": nn.Softplus(),\n",
    "        \"identity\": nn.Identity(),\n",
    "    }\n",
    "    def __init__(self, hidden_dims, ob_dim, ac_dim, activation=\"relu\", output_activation='identity'):\n",
    "        super().__init__()\n",
    "        self.ac_dim = ac_dim\n",
    "        self.ob_dim = ob_dim\n",
    "        activation = self._str_to_activation[activation]\n",
    "        output_activation = self._str_to_activation[output_activation]\n",
    "        layers = []\n",
    "        hidden_dims = [ob_dim + ac_dim] + hidden_dims\n",
    "        for n in range(len(hidden_dims) - 1):\n",
    "            layers.append(nn.Linear(hidden_dims[n], hidden_dims[n+1]))\n",
    "            layers.append(activation)\n",
    "        layers.append(nn.Linear(hidden_dims[-1], ob_dim))\n",
    "        layers.append(output_activation)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "    \n",
    "    def update_action(self, actions: torch.Tensor, times: torch.Tensor):\n",
    "        ep_len = actions.shape[0]\n",
    "        assert actions.shape == (ep_len, self.ac_dim) and times.shape == (ep_len,)\n",
    "        # times = times - times[0] # start with t=0\n",
    "        # right now, do not assume t0 = 0\n",
    "        self.register_buffer(\"times\", times)\n",
    "        self.register_buffer(\"actions\", actions)\n",
    "\n",
    "    def _get_action(self, t):\n",
    "        idx = torch.searchsorted(self.times, t, right=True) - 1\n",
    "        return self.actions[idx]\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        ac = self._get_action(t)\n",
    "        return self.net(torch.cat((y, ac), dim=-1))\n",
    "\n",
    "    \n",
    "class ODEAgent(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        hidden_dims: Sequence[int],\n",
    "        make_optimizer: Callable[[nn.ParameterList], torch.optim.Optimizer],\n",
    "        ensemble_size: int,\n",
    "        mpc_horizon_steps: int,\n",
    "        mpc_timestep: float,\n",
    "        mpc_strategy: str,\n",
    "        mpc_num_action_sequences: int,\n",
    "        cem_num_iters: Optional[int] = None,\n",
    "        cem_num_elites: Optional[int] = None,\n",
    "        cem_alpha: Optional[float] = None,\n",
    "        activation: str = \"relu\",\n",
    "        output_activation: str = \"identity\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.mpc_horizon_steps = mpc_horizon_steps # in terms of timesteps\n",
    "        self.mpc_strategy = mpc_strategy\n",
    "        self.mpc_num_action_sequences = mpc_num_action_sequences\n",
    "        self.cem_num_iters = cem_num_iters\n",
    "        self.cem_num_elites = cem_num_elites\n",
    "        self.cem_alpha = cem_alpha\n",
    "        self.mpc_timestep = mpc_timestep # when evaluating\n",
    "\n",
    "        assert mpc_strategy in (\n",
    "            \"random\",\n",
    "            \"cem\",\n",
    "        ), f\"'{mpc_strategy}' is not a valid MPC strategy\"\n",
    "\n",
    "        # ensure the environment is state-based\n",
    "        assert len(env.observation_space.shape) == 1\n",
    "        assert len(env.action_space.shape) == 1\n",
    "\n",
    "        self.ob_dim = env.observation_space.shape[0]\n",
    "        self.ac_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.ode_functions = nn.ModuleList(\n",
    "            [\n",
    "                NeuralODE(\n",
    "                    hidden_dims,\n",
    "                    self.ob_dim,\n",
    "                    self.ac_dim,\n",
    "                    activation,\n",
    "                    output_activation\n",
    "                ).to(ptu.device)\n",
    "                for _ in range(ensemble_size)\n",
    "            ]\n",
    "        )\n",
    "        self.optimizer = make_optimizer(self.ode_functions.parameters())\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def update(self, i: int, obs: np.ndarray, acs: np.ndarray, times: np.ndarray):\n",
    "        \"\"\"\n",
    "        Update self.dynamics_models[i] using the given trajectory\n",
    "\n",
    "        Args:\n",
    "            i: index of the dynamics model to update\n",
    "            obs: (ep_len, ob_dim)\n",
    "            acs: (ep_len, ac_dim)\n",
    "            times: (ep_len)\n",
    "        \"\"\"\n",
    "        obs = ptu.from_numpy(obs)\n",
    "        acs = ptu.from_numpy(acs)\n",
    "        times = ptu.from_numpy(times)\n",
    "        ode_func = self.ode_functions[i]\n",
    "        ode_func.update_action(acs, times)\n",
    "        ode_out = odeint(ode_func, obs[0, :], times) # t0 = times[0] in torchdiffeq\n",
    "        # possible problem: the ode function is only \"evaluating\" on times\n",
    "        # I am not sure whether there is an implicit dt or dt[i] = times[i+1] - times[i]\n",
    "        # I know for diffrax in jax, there is a separate dt argument passed into odeint()\n",
    "        assert ode_out.shape == obs.shape\n",
    "        loss = self.loss_fn(ode_out, obs)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return ptu.to_numpy(loss)\n",
    "    \n",
    "    def update_statistics(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_action_sequences(self, obs: np.ndarray, acs: np.ndarray):\n",
    "        obs = ptu.from_numpy(obs) # (ob_dim)\n",
    "        acs_np = acs\n",
    "        acs = ptu.from_numpy(acs) # (N, steps, ac_dim)\n",
    "        times = torch.linspace(0, (self.mpc_horizon_steps - 1) * self.mpc_timestep, self.mpc_horizon_steps, device=ptu.device)\n",
    "        reward_arr = np.zeros((self.mpc_num_action_sequences, self.ensemble_size))\n",
    "        for n in range(self.mpc_num_action_sequences):\n",
    "            for i in range(self.ensemble_size):\n",
    "                ode_func = self.ode_functions[i]\n",
    "                ode_func.update_action(acs[n, :, :], times)\n",
    "                ode_out = odeint(ode_func, obs, times) # (steps, ob_dim)\n",
    "                rewards, _ = self.env.get_reward(ptu.to_numpy(ode_out), acs_np[n, :, :])\n",
    "                avg_reward = np.mean(rewards)\n",
    "                reward_arr[n, i] = avg_reward\n",
    "        return np.mean(reward_arr, axis=1)\n",
    "    # maybe I should manually implement batched Euler solver\n",
    "    # to make inference faster\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_action(self, obs: np.ndarray):\n",
    "        \"\"\"\n",
    "        Choose the best action using model-predictive control.\n",
    "\n",
    "        Args:\n",
    "            obs: (ob_dim,)\n",
    "        \"\"\"\n",
    "        # always start with uniformly random actions\n",
    "        actions = np.random.uniform(\n",
    "            self.env.action_space.low,\n",
    "            self.env.action_space.high,\n",
    "            size=(self.mpc_num_action_sequences, self.mpc_horizon_steps, self.ac_dim),\n",
    "        )\n",
    "\n",
    "        if self.mpc_strategy == \"random\":\n",
    "            # evaluate each action sequence and return the best one\n",
    "            rewards = self.evaluate_action_sequences(obs, actions)\n",
    "            assert rewards.shape == (self.mpc_num_action_sequences,)\n",
    "            best_index = np.argmax(rewards)\n",
    "            return actions[best_index, 0, :]\n",
    "        elif self.mpc_strategy == \"cem\":\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid MPC strategy '{self.mpc_strategy}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_sampler = ConstantSampler(dt=0.05)\n",
    "env = PendulumEnv(\n",
    "    dt_sampler=dt_sampler\n",
    ")\n",
    "mb_agent = ODEAgent(\n",
    "    env=env,\n",
    "    hidden_dims=[128, 128, 128],\n",
    "    make_optimizer=lambda param_list: torch.optim.AdamW(param_list),\n",
    "    ensemble_size=10,\n",
    "    mpc_horizon_steps=100,\n",
    "    mpc_timestep=0.005,\n",
    "    mpc_strategy=\"random\",\n",
    "    mpc_num_action_sequences=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.random.uniform(\n",
    "    mb_agent.env.action_space.low,\n",
    "    mb_agent.env.action_space.high,\n",
    "    size=(mb_agent.mpc_num_action_sequences, mb_agent.mpc_horizon_steps, mb_agent.ac_dim),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 82.8017 s\n",
      "File: /tmp/ipykernel_186605/3267144233.py\n",
      "Function: evaluate_action_sequences at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def evaluate_action_sequences(agent, obs: np.ndarray, acs: np.ndarray):\n",
      "     2         1      13825.0  13825.0      0.0      with torch.no_grad():\n",
      "     3         1      24795.0  24795.0      0.0          obs = ptu.from_numpy(obs)\n",
      "     4         1        116.0    116.0      0.0          acs_np = acs\n",
      "     5         1      83303.0  83303.0      0.0          acs = ptu.from_numpy(acs)\n",
      "     6         1      24536.0  24536.0      0.0          times = torch.linspace(0, (agent.mpc_horizon_steps - 1) * agent.mpc_timestep, agent.mpc_horizon_steps, device=ptu.device)\n",
      "     7         1       6850.0   6850.0      0.0          reward_arr = np.zeros((agent.mpc_num_action_sequences, agent.ensemble_size))\n",
      "     8        11       2248.0    204.4      0.0          for n in range(agent.mpc_num_action_sequences):\n",
      "     9       110      34870.0    317.0      0.0              for i in range(agent.ensemble_size):\n",
      "    10       100     832289.0   8322.9      0.0                  ode_func = agent.ode_functions[i]\n",
      "    11       100    1429678.0  14296.8      0.0                  ode_func.update_action(acs[n, :, :], times)\n",
      "    12       100        8e+10    8e+08    100.0                  ode_out = odeint(ode_func, obs, times) # (steps, ob_dim)\n",
      "    13       100    6478946.0  64789.5      0.0                  rewards, _ = agent.env.get_reward(ptu.to_numpy(ode_out), acs_np[n, :, :])\n",
      "    14       100    3020976.0  30209.8      0.0                  avg_reward = np.mean(rewards)\n",
      "    15       100      68506.0    685.1      0.0                  reward_arr[n, i] = avg_reward\n",
      "    16         1      26776.0  26776.0      0.0          return np.mean(reward_arr, axis=1)"
     ]
    }
   ],
   "source": [
    "%lprun -f evaluate_action_sequences evaluate_action_sequences(mb_agent, ob, actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
